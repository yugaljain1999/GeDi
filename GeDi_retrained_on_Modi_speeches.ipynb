{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GeDi retrained on Modi speeches.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yugaljain1999/GeDi/blob/master/GeDi_retrained_on_Modi_speeches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kceCz5DQHWIV"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/salesforce/GeDi/blob/master/GeDi_guided_GPT_2_XL.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpqIppnIj1UH",
        "outputId": "99d0980d-1434-40bd-c60f-5aac08c6bdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! git clone https://github.com/salesforce/GeDi/\n",
        "! pip install transformers==2.8.0\n",
        "! cd content\n",
        "! git clone https://github.com/NVIDIA/apex\n",
        "! cd apex\n",
        "! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" /content/apex\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "! cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GeDi' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.15.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.19.0,>=1.18.11 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.18.11)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.11->boto3->transformers==2.8.0) (2.8.1)\n",
            "/bin/bash: line 0: cd: content: No such file or directory\n",
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-ty10po64\n",
            "Created temporary directory: /tmp/pip-req-tracker-h_rf1sqh\n",
            "Created requirements tracker '/tmp/pip-req-tracker-h_rf1sqh'\n",
            "Created temporary directory: /tmp/pip-install-kgsqdzw9\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-sqto8c8j\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-h_rf1sqh'\n",
            "    Running setup.py (path:/tmp/pip-req-build-sqto8c8j/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-sqto8c8j/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-sqto8c8j/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-sqto8c8j/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-sqto8c8j/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-sqto8c8j/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-sqto8c8j/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-sqto8c8j/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-sqto8c8j has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-h_rf1sqh'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-8m4jhm6t\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-sqto8c8j/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-sqto8c8j/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-8m4jhm6t/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-sqto8c8j/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:335: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:146:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:268:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/record_function.h(18): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(126): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/record_function.h(18): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(126): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-8m4jhm6t/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-sqto8c8j\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-h_rf1sqh'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNJeqiYXlSX8",
        "outputId": "bb136633-f9db-4311-a0c5-b701792f593d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "modi = pd.read_csv('/content/modi_speeches_final (3).csv')\n",
        "text = []\n",
        "keys = [\"science\", \"public health\", \"economics\",\"crime\",\"world\",\"sports\",\"religion\",\"business\",\"elections\"]\n",
        "values = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "dict_ = {}\n",
        "for k,v in zip(keys,values):\n",
        "  dict_[k] = v\n",
        "for i in range(0,len(modi)):\n",
        "  text.append(modi['speech'][i])\n",
        "text = pd.Series(text)\n",
        "modi = modi.drop(['Unnamed: 0','lang'],axis=1)\n",
        "modi['topics'] = modi['topics'].map(dict_)\n",
        "import numpy as np\n",
        "modi['na'] = np.nan\n",
        "modi.na = modi['speech']\n",
        "modi.speech = modi['topics']\n",
        "modi.topics =np.nan\n",
        "modi.na = pd.read_csv('/content/modi_speeches_final (3).csv')['speech']\n",
        "modi.na = text\n",
        "modi.to_csv('modi_speech_final.csv')\n",
        "modi.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speech</th>\n",
              "      <th>topics</th>\n",
              "      <th>na</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My dear countrymen, Namaskar.\\nGenerally, this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our country’s Agriculture Minister Shri Narend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My cabinet colleague, Shri Rajnath ji, Chief o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My dear countrymen,\\nCongratulations and many ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The process of Structural Reforms going on in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  speech  topics                                                 na\n",
              "0      7     NaN  My dear countrymen, Namaskar.\\nGenerally, this...\n",
              "1      8     NaN  Our country’s Agriculture Minister Shri Narend...\n",
              "2      5     NaN  My cabinet colleague, Shri Rajnath ji, Chief o...\n",
              "3      5     NaN  My dear countrymen,\\nCongratulations and many ...\n",
              "4      5     NaN  The process of Structural Reforms going on in ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULhROwnOIYhO"
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "modi_csv = pd.read_csv('modi_speech_final.csv')\n",
        "modi_csv = modi_csv.drop('Unnamed: 0',axis=1)\n",
        "train,test = train_test_split(modi_csv,test_size=0.3)\n",
        "train.to_csv('/content/GeDi/data/AG-news/train.csv',header=None,index=None)\n",
        "test.to_csv(\"/content/GeDi/data/AG-news/test.csv\",header=None,index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wUbBNKvJKo_",
        "outputId": "6d9fedd2-52a5-4ab0-84da-60aab37c0900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!bash /content/GeDi/scripts/get_models.sh\n",
        "!cd /content/GeDi\n",
        "!cd /content/GeDi/scripts\n",
        "!bash /content/GeDi/scripts/get_data.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "skipping 9031\n",
            "skipping 9032\n",
            "skipping 9033\n",
            "skipping 9034\n",
            "skipping 9035\n",
            "skipping 9036\n",
            "skipping 9037\n",
            "skipping 9038\n",
            "skipping 9039\n",
            "skipping 9040\n",
            "skipping 9041\n",
            "skipping 9042\n",
            "skipping 9044\n",
            "skipping 9045\n",
            "skipping 9046\n",
            "skipping 9047\n",
            "skipping 9048\n",
            "skipping 9049\n",
            "skipping 9050\n",
            "skipping 9051\n",
            "skipping 9052\n",
            "skipping 9053\n",
            "skipping 9054\n",
            "skipping 9055\n",
            "skipping 9056\n",
            "skipping 9057\n",
            "skipping 9058\n",
            "skipping 9059\n",
            "skipping 9060\n",
            "skipping 9061\n",
            "skipping 9062\n",
            "skipping 9063\n",
            "skipping 9064\n",
            "skipping 9065\n",
            "skipping 9066\n",
            "skipping 9067\n",
            "skipping 9068\n",
            "skipping 9069\n",
            "skipping 9070\n",
            "skipping 9071\n",
            "skipping 9072\n",
            "skipping 9073\n",
            "skipping 9074\n",
            "skipping 9075\n",
            "skipping 9076\n",
            "skipping 9077\n",
            "skipping 9078\n",
            "skipping 9079\n",
            "skipping 9080\n",
            "skipping 9081\n",
            "skipping 9082\n",
            "skipping 9083\n",
            "skipping 9084\n",
            "skipping 9085\n",
            "skipping 9086\n",
            "skipping 9087\n",
            "skipping 9088\n",
            "skipping 9089\n",
            "skipping 9090\n",
            "skipping 9091\n",
            "skipping 9092\n",
            "skipping 9093\n",
            "skipping 9094\n",
            "skipping 9095\n",
            "skipping 9096\n",
            "skipping 9097\n",
            "skipping 9098\n",
            "skipping 9099\n",
            "skipping 9100\n",
            "skipping 9101\n",
            "skipping 9102\n",
            "skipping 9103\n",
            "skipping 9104\n",
            "skipping 9105\n",
            "skipping 9106\n",
            "skipping 9107\n",
            "skipping 9108\n",
            "skipping 9109\n",
            "skipping 9110\n",
            "skipping 9111\n",
            "skipping 9112\n",
            "skipping 9113\n",
            "skipping 9114\n",
            "skipping 9115\n",
            "skipping 9116\n",
            "skipping 9117\n",
            "skipping 9118\n",
            "skipping 9119\n",
            "skipping 9120\n",
            "skipping 9121\n",
            "skipping 9122\n",
            "skipping 9123\n",
            "skipping 9124\n",
            "skipping 9125\n",
            "skipping 9126\n",
            "skipping 9127\n",
            "skipping 9128\n",
            "skipping 9129\n",
            "skipping 9130\n",
            "skipping 9131\n",
            "skipping 9132\n",
            "skipping 9133\n",
            "skipping 9134\n",
            "skipping 9135\n",
            "skipping 9136\n",
            "skipping 9137\n",
            "skipping 9138\n",
            "skipping 9139\n",
            "skipping 9140\n",
            "skipping 9141\n",
            "skipping 9142\n",
            "skipping 9143\n",
            "skipping 9144\n",
            "skipping 9145\n",
            "skipping 9146\n",
            "skipping 9147\n",
            "skipping 9148\n",
            "skipping 9150\n",
            "skipping 9151\n",
            "skipping 9152\n",
            "skipping 9153\n",
            "skipping 9154\n",
            "skipping 9155\n",
            "skipping 9156\n",
            "skipping 9157\n",
            "skipping 9158\n",
            "skipping 9159\n",
            "skipping 9160\n",
            "skipping 9161\n",
            "skipping 9162\n",
            "skipping 9163\n",
            "skipping 9164\n",
            "skipping 9165\n",
            "skipping 9166\n",
            "skipping 9167\n",
            "skipping 9168\n",
            "skipping 9169\n",
            "skipping 9170\n",
            "skipping 9171\n",
            "skipping 9172\n",
            "skipping 9173\n",
            "skipping 9174\n",
            "skipping 9175\n",
            "skipping 9177\n",
            "skipping 9178\n",
            "skipping 9179\n",
            "skipping 9180\n",
            "skipping 9181\n",
            "skipping 9182\n",
            "skipping 9183\n",
            "skipping 9184\n",
            "skipping 9185\n",
            "skipping 9186\n",
            "skipping 9187\n",
            "skipping 9188\n",
            "skipping 9189\n",
            "skipping 9190\n",
            "skipping 9191\n",
            "skipping 9192\n",
            "skipping 9193\n",
            "skipping 9194\n",
            "skipping 9195\n",
            "skipping 9196\n",
            "skipping 9197\n",
            "skipping 9198\n",
            "skipping 9199\n",
            "skipping 9200\n",
            "skipping 9201\n",
            "skipping 9202\n",
            "skipping 9203\n",
            "skipping 9204\n",
            "skipping 9206\n",
            "skipping 9207\n",
            "skipping 9208\n",
            "skipping 9209\n",
            "skipping 9210\n",
            "skipping 9211\n",
            "skipping 9212\n",
            "skipping 9213\n",
            "skipping 9214\n",
            "skipping 9215\n",
            "skipping 9216\n",
            "skipping 9217\n",
            "skipping 9218\n",
            "skipping 9219\n",
            "skipping 9220\n",
            "skipping 9221\n",
            "skipping 9222\n",
            "skipping 9223\n",
            "skipping 9224\n",
            "skipping 9225\n",
            "skipping 9226\n",
            "skipping 9227\n",
            "skipping 9228\n",
            "skipping 9229\n",
            "skipping 9230\n",
            "skipping 9231\n",
            "skipping 9232\n",
            "skipping 9233\n",
            "skipping 9234\n",
            "skipping 9235\n",
            "skipping 9236\n",
            "skipping 9237\n",
            "skipping 9238\n",
            "skipping 9239\n",
            "skipping 9240\n",
            "skipping 9241\n",
            "skipping 9242\n",
            "skipping 9244\n",
            "skipping 9245\n",
            "skipping 9246\n",
            "skipping 9247\n",
            "skipping 9248\n",
            "skipping 9249\n",
            "skipping 9250\n",
            "skipping 9251\n",
            "skipping 9252\n",
            "skipping 9253\n",
            "skipping 9254\n",
            "skipping 9255\n",
            "skipping 9256\n",
            "skipping 9257\n",
            "skipping 9258\n",
            "skipping 9259\n",
            "skipping 9260\n",
            "skipping 9261\n",
            "skipping 9262\n",
            "skipping 9263\n",
            "skipping 9264\n",
            "skipping 9265\n",
            "skipping 9266\n",
            "skipping 9267\n",
            "skipping 9268\n",
            "skipping 9269\n",
            "skipping 9270\n",
            "skipping 9271\n",
            "skipping 9272\n",
            "skipping 9273\n",
            "skipping 9274\n",
            "skipping 9275\n",
            "skipping 9276\n",
            "skipping 9277\n",
            "skipping 9278\n",
            "skipping 9279\n",
            "skipping 9280\n",
            "skipping 9281\n",
            "skipping 9282\n",
            "skipping 9283\n",
            "skipping 9284\n",
            "skipping 9285\n",
            "skipping 9286\n",
            "skipping 9288\n",
            "skipping 9289\n",
            "skipping 9290\n",
            "skipping 9291\n",
            "skipping 9292\n",
            "skipping 9293\n",
            "skipping 9294\n",
            "skipping 9295\n",
            "skipping 9296\n",
            "skipping 9297\n",
            "skipping 9298\n",
            "skipping 9299\n",
            "skipping 9300\n",
            "skipping 9301\n",
            "skipping 9302\n",
            "skipping 9303\n",
            "skipping 9304\n",
            "skipping 9305\n",
            "skipping 9306\n",
            "skipping 9307\n",
            "skipping 9308\n",
            "skipping 9309\n",
            "skipping 9310\n",
            "skipping 9311\n",
            "skipping 9312\n",
            "skipping 9313\n",
            "skipping 9314\n",
            "skipping 9315\n",
            "skipping 9316\n",
            "skipping 9317\n",
            "skipping 9318\n",
            "skipping 9319\n",
            "skipping 9320\n",
            "skipping 9321\n",
            "skipping 9322\n",
            "skipping 9324\n",
            "skipping 9325\n",
            "skipping 9326\n",
            "skipping 9327\n",
            "skipping 9328\n",
            "skipping 9329\n",
            "skipping 9330\n",
            "skipping 9331\n",
            "skipping 9332\n",
            "skipping 9333\n",
            "skipping 9334\n",
            "skipping 9335\n",
            "skipping 9336\n",
            "skipping 9337\n",
            "skipping 9338\n",
            "skipping 9339\n",
            "skipping 9340\n",
            "skipping 9341\n",
            "skipping 9342\n",
            "skipping 9343\n",
            "skipping 9344\n",
            "skipping 9345\n",
            "skipping 9346\n",
            "skipping 9347\n",
            "skipping 9348\n",
            "skipping 9349\n",
            "skipping 9350\n",
            "skipping 9351\n",
            "skipping 9352\n",
            "skipping 9353\n",
            "skipping 9354\n",
            "skipping 9355\n",
            "skipping 9356\n",
            "skipping 9357\n",
            "skipping 9358\n",
            "skipping 9359\n",
            "skipping 9360\n",
            "skipping 9361\n",
            "skipping 9362\n",
            "skipping 9363\n",
            "skipping 9364\n",
            "skipping 9365\n",
            "skipping 9366\n",
            "skipping 9367\n",
            "skipping 9368\n",
            "skipping 9369\n",
            "skipping 9370\n",
            "skipping 9371\n",
            "skipping 9372\n",
            "skipping 9374\n",
            "skipping 9375\n",
            "skipping 9376\n",
            "skipping 9377\n",
            "skipping 9378\n",
            "skipping 9379\n",
            "skipping 9380\n",
            "skipping 9381\n",
            "skipping 9382\n",
            "skipping 9383\n",
            "skipping 9384\n",
            "skipping 9385\n",
            "skipping 9386\n",
            "skipping 9387\n",
            "skipping 9388\n",
            "skipping 9389\n",
            "skipping 9390\n",
            "skipping 9391\n",
            "skipping 9392\n",
            "skipping 9393\n",
            "skipping 9394\n",
            "skipping 9395\n",
            "skipping 9397\n",
            "skipping 9398\n",
            "skipping 9399\n",
            "skipping 9400\n",
            "skipping 9401\n",
            "skipping 9402\n",
            "skipping 9403\n",
            "skipping 9404\n",
            "skipping 9405\n",
            "skipping 9406\n",
            "skipping 9407\n",
            "skipping 9408\n",
            "skipping 9409\n",
            "skipping 9410\n",
            "skipping 9411\n",
            "skipping 9412\n",
            "skipping 9413\n",
            "skipping 9414\n",
            "skipping 9415\n",
            "skipping 9416\n",
            "skipping 9417\n",
            "skipping 9418\n",
            "skipping 9419\n",
            "skipping 9420\n",
            "skipping 9421\n",
            "skipping 9422\n",
            "skipping 9423\n",
            "skipping 9424\n",
            "skipping 9425\n",
            "skipping 9426\n",
            "skipping 9427\n",
            "skipping 9428\n",
            "skipping 9429\n",
            "skipping 9430\n",
            "skipping 9431\n",
            "skipping 9432\n",
            "skipping 9433\n",
            "skipping 9434\n",
            "skipping 9435\n",
            "skipping 9436\n",
            "skipping 9437\n",
            "skipping 9438\n",
            "skipping 9439\n",
            "skipping 9440\n",
            "skipping 9441\n",
            "skipping 9442\n",
            "skipping 9443\n",
            "skipping 9444\n",
            "skipping 9445\n",
            "skipping 9446\n",
            "skipping 9447\n",
            "skipping 9448\n",
            "skipping 9449\n",
            "skipping 9450\n",
            "skipping 9451\n",
            "skipping 9452\n",
            "skipping 9453\n",
            "skipping 9454\n",
            "skipping 9455\n",
            "skipping 9456\n",
            "skipping 9457\n",
            "skipping 9458\n",
            "skipping 9459\n",
            "skipping 9460\n",
            "skipping 9461\n",
            "skipping 9462\n",
            "skipping 9463\n",
            "skipping 9464\n",
            "skipping 9465\n",
            "skipping 9466\n",
            "skipping 9467\n",
            "skipping 9468\n",
            "skipping 9469\n",
            "skipping 9470\n",
            "skipping 9471\n",
            "skipping 9472\n",
            "skipping 9473\n",
            "skipping 9474\n",
            "skipping 9475\n",
            "skipping 9476\n",
            "skipping 9477\n",
            "skipping 9478\n",
            "skipping 9479\n",
            "skipping 9480\n",
            "skipping 9481\n",
            "skipping 9482\n",
            "skipping 9483\n",
            "skipping 9484\n",
            "skipping 9485\n",
            "skipping 9486\n",
            "skipping 9487\n",
            "skipping 9488\n",
            "skipping 9489\n",
            "skipping 9490\n",
            "skipping 9491\n",
            "skipping 9492\n",
            "skipping 9493\n",
            "skipping 9494\n",
            "skipping 9495\n",
            "skipping 9496\n",
            "skipping 9497\n",
            "skipping 9498\n",
            "skipping 9499\n",
            "skipping 9500\n",
            "skipping 9502\n",
            "skipping 9503\n",
            "skipping 9504\n",
            "skipping 9505\n",
            "skipping 9506\n",
            "skipping 9507\n",
            "skipping 9508\n",
            "skipping 9509\n",
            "skipping 9510\n",
            "skipping 9511\n",
            "skipping 9512\n",
            "skipping 9513\n",
            "skipping 9514\n",
            "skipping 9515\n",
            "skipping 9516\n",
            "skipping 9517\n",
            "skipping 9518\n",
            "skipping 9519\n",
            "skipping 9520\n",
            "skipping 9521\n",
            "skipping 9522\n",
            "skipping 9523\n",
            "skipping 9524\n",
            "skipping 9525\n",
            "skipping 9526\n",
            "skipping 9528\n",
            "skipping 9529\n",
            "skipping 9530\n",
            "skipping 9531\n",
            "skipping 9532\n",
            "skipping 9533\n",
            "skipping 9534\n",
            "skipping 9535\n",
            "skipping 9536\n",
            "skipping 9537\n",
            "skipping 9538\n",
            "skipping 9539\n",
            "skipping 9541\n",
            "skipping 9542\n",
            "skipping 9543\n",
            "skipping 9544\n",
            "skipping 9545\n",
            "skipping 9546\n",
            "skipping 9547\n",
            "skipping 9548\n",
            "skipping 9549\n",
            "skipping 9550\n",
            "skipping 9551\n",
            "skipping 9552\n",
            "skipping 9553\n",
            "skipping 9554\n",
            "skipping 9555\n",
            "skipping 9556\n",
            "skipping 9557\n",
            "skipping 9558\n",
            "skipping 9559\n",
            "skipping 9560\n",
            "skipping 9561\n",
            "skipping 9562\n",
            "skipping 9563\n",
            "skipping 9564\n",
            "skipping 9565\n",
            "skipping 9566\n",
            "skipping 9567\n",
            "skipping 9568\n",
            "skipping 9569\n",
            "skipping 9571\n",
            "skipping 9572\n",
            "skipping 9573\n",
            "skipping 9574\n",
            "skipping 9575\n",
            "skipping 9576\n",
            "skipping 9577\n",
            "skipping 9578\n",
            "skipping 9579\n",
            "skipping 9580\n",
            "skipping 9581\n",
            "skipping 9582\n",
            "skipping 9583\n",
            "skipping 9584\n",
            "skipping 9585\n",
            "skipping 9586\n",
            "skipping 9587\n",
            "skipping 9588\n",
            "skipping 9589\n",
            "skipping 9590\n",
            "skipping 9591\n",
            "skipping 9592\n",
            "skipping 9593\n",
            "skipping 9594\n",
            "skipping 9595\n",
            "skipping 9596\n",
            "skipping 9597\n",
            "skipping 9598\n",
            "skipping 9599\n",
            "skipping 9600\n",
            "skipping 9601\n",
            "skipping 9602\n",
            "skipping 9603\n",
            "skipping 9604\n",
            "skipping 9606\n",
            "skipping 9607\n",
            "skipping 9608\n",
            "skipping 9609\n",
            "skipping 9610\n",
            "skipping 9611\n",
            "skipping 9612\n",
            "skipping 9613\n",
            "skipping 9614\n",
            "skipping 9615\n",
            "skipping 9616\n",
            "skipping 9617\n",
            "skipping 9618\n",
            "skipping 9619\n",
            "skipping 9620\n",
            "skipping 9621\n",
            "skipping 9622\n",
            "skipping 9623\n",
            "skipping 9624\n",
            "skipping 9625\n",
            "skipping 9626\n",
            "skipping 9627\n",
            "skipping 9628\n",
            "skipping 9629\n",
            "skipping 9630\n",
            "skipping 9631\n",
            "skipping 9632\n",
            "skipping 9633\n",
            "skipping 9634\n",
            "skipping 9635\n",
            "skipping 9636\n",
            "skipping 9637\n",
            "skipping 9638\n",
            "skipping 9639\n",
            "skipping 9640\n",
            "skipping 9641\n",
            "skipping 9642\n",
            "skipping 9643\n",
            "skipping 9644\n",
            "skipping 9645\n",
            "skipping 9646\n",
            "skipping 9647\n",
            "skipping 9648\n",
            "skipping 9649\n",
            "skipping 9650\n",
            "skipping 9651\n",
            "skipping 9652\n",
            "skipping 9653\n",
            "skipping 9654\n",
            "skipping 9655\n",
            "skipping 9656\n",
            "skipping 9657\n",
            "skipping 9658\n",
            "skipping 9659\n",
            "skipping 9660\n",
            "skipping 9661\n",
            "skipping 9662\n",
            "skipping 9663\n",
            "skipping 9664\n",
            "skipping 9665\n",
            "skipping 9666\n",
            "skipping 9667\n",
            "skipping 9668\n",
            "skipping 9669\n",
            "skipping 9670\n",
            "skipping 9671\n",
            "skipping 9672\n",
            "skipping 9673\n",
            "skipping 9674\n",
            "skipping 9675\n",
            "skipping 9676\n",
            "skipping 9677\n",
            "skipping 9678\n",
            "skipping 9679\n",
            "skipping 9680\n",
            "skipping 9681\n",
            "skipping 9682\n",
            "skipping 9684\n",
            "skipping 9685\n",
            "skipping 9686\n",
            "skipping 9687\n",
            "skipping 9688\n",
            "skipping 9689\n",
            "skipping 9690\n",
            "skipping 9691\n",
            "skipping 9692\n",
            "skipping 9693\n",
            "skipping 9694\n",
            "skipping 9695\n",
            "skipping 9696\n",
            "skipping 9697\n",
            "skipping 9698\n",
            "skipping 9699\n",
            "skipping 9700\n",
            "skipping 9701\n",
            "skipping 9702\n",
            "skipping 9703\n",
            "skipping 9704\n",
            "skipping 9705\n",
            "skipping 9706\n",
            "skipping 9707\n",
            "skipping 9708\n",
            "skipping 9709\n",
            "skipping 9710\n",
            "skipping 9711\n",
            "skipping 9712\n",
            "skipping 9713\n",
            "skipping 9714\n",
            "skipping 9715\n",
            "skipping 9716\n",
            "skipping 9717\n",
            "skipping 9718\n",
            "skipping 9719\n",
            "skipping 9721\n",
            "skipping 9722\n",
            "skipping 9723\n",
            "skipping 9724\n",
            "skipping 9725\n",
            "skipping 9726\n",
            "skipping 9727\n",
            "skipping 9728\n",
            "skipping 9729\n",
            "skipping 9730\n",
            "skipping 9731\n",
            "skipping 9732\n",
            "skipping 9733\n",
            "skipping 9734\n",
            "skipping 9735\n",
            "skipping 9736\n",
            "skipping 9737\n",
            "skipping 9738\n",
            "skipping 9739\n",
            "skipping 9740\n",
            "skipping 9741\n",
            "skipping 9742\n",
            "skipping 9743\n",
            "skipping 9744\n",
            "skipping 9745\n",
            "skipping 9746\n",
            "skipping 9747\n",
            "skipping 9748\n",
            "skipping 9749\n",
            "skipping 9750\n",
            "skipping 9751\n",
            "skipping 9752\n",
            "skipping 9753\n",
            "skipping 9754\n",
            "skipping 9755\n",
            "skipping 9756\n",
            "skipping 9757\n",
            "skipping 9758\n",
            "skipping 9759\n",
            "skipping 9760\n",
            "skipping 9761\n",
            "skipping 9762\n",
            "skipping 9763\n",
            "skipping 9764\n",
            "skipping 9765\n",
            "skipping 9766\n",
            "skipping 9767\n",
            "skipping 9768\n",
            "skipping 9769\n",
            "skipping 9770\n",
            "skipping 9771\n",
            "skipping 9772\n",
            "skipping 9773\n",
            "skipping 9774\n",
            "skipping 9775\n",
            "skipping 9776\n",
            "skipping 9777\n",
            "skipping 9778\n",
            "skipping 9779\n",
            "skipping 9780\n",
            "skipping 9781\n",
            "skipping 9782\n",
            "skipping 9783\n",
            "skipping 9784\n",
            "skipping 9785\n",
            "skipping 9786\n",
            "skipping 9787\n",
            "skipping 9788\n",
            "skipping 9789\n",
            "skipping 9790\n",
            "skipping 9791\n",
            "skipping 9792\n",
            "skipping 9793\n",
            "skipping 9794\n",
            "skipping 9795\n",
            "skipping 9796\n",
            "skipping 9797\n",
            "skipping 9798\n",
            "skipping 9799\n",
            "skipping 9800\n",
            "skipping 9801\n",
            "skipping 9802\n",
            "skipping 9803\n",
            "skipping 9804\n",
            "skipping 9805\n",
            "skipping 9806\n",
            "skipping 9807\n",
            "skipping 9808\n",
            "skipping 9809\n",
            "skipping 9810\n",
            "skipping 9811\n",
            "skipping 9812\n",
            "skipping 9813\n",
            "skipping 9814\n",
            "skipping 9815\n",
            "skipping 9816\n",
            "skipping 9817\n",
            "skipping 9818\n",
            "skipping 9819\n",
            "skipping 9820\n",
            "skipping 9821\n",
            "skipping 9822\n",
            "skipping 9823\n",
            "skipping 9824\n",
            "skipping 9825\n",
            "skipping 9826\n",
            "skipping 9827\n",
            "skipping 9828\n",
            "skipping 9829\n",
            "skipping 9830\n",
            "skipping 9831\n",
            "skipping 9832\n",
            "skipping 9833\n",
            "skipping 9834\n",
            "skipping 9835\n",
            "skipping 9836\n",
            "skipping 9837\n",
            "skipping 9838\n",
            "skipping 9839\n",
            "skipping 9840\n",
            "skipping 9841\n",
            "skipping 9842\n",
            "skipping 9843\n",
            "skipping 9844\n",
            "skipping 9845\n",
            "skipping 9846\n",
            "skipping 9847\n",
            "skipping 9848\n",
            "skipping 9849\n",
            "skipping 9850\n",
            "skipping 9851\n",
            "skipping 9852\n",
            "skipping 9853\n",
            "skipping 9854\n",
            "skipping 9855\n",
            "skipping 9856\n",
            "skipping 9857\n",
            "skipping 9858\n",
            "skipping 9859\n",
            "skipping 9860\n",
            "skipping 9861\n",
            "skipping 9862\n",
            "skipping 9863\n",
            "skipping 9864\n",
            "skipping 9865\n",
            "skipping 9866\n",
            "skipping 9867\n",
            "skipping 9868\n",
            "skipping 9869\n",
            "skipping 9870\n",
            "skipping 9871\n",
            "skipping 9872\n",
            "skipping 9873\n",
            "skipping 9874\n",
            "skipping 9875\n",
            "skipping 9876\n",
            "skipping 9877\n",
            "skipping 9878\n",
            "skipping 9879\n",
            "skipping 9880\n",
            "skipping 9881\n",
            "skipping 9882\n",
            "skipping 9883\n",
            "skipping 9884\n",
            "skipping 9885\n",
            "skipping 9886\n",
            "skipping 9887\n",
            "skipping 9888\n",
            "skipping 9889\n",
            "skipping 9890\n",
            "skipping 9891\n",
            "skipping 9892\n",
            "skipping 9893\n",
            "skipping 9894\n",
            "skipping 9895\n",
            "skipping 9896\n",
            "skipping 9897\n",
            "skipping 9898\n",
            "skipping 9899\n",
            "skipping 9900\n",
            "skipping 9901\n",
            "skipping 9902\n",
            "skipping 9903\n",
            "skipping 9904\n",
            "skipping 9905\n",
            "skipping 9906\n",
            "skipping 9907\n",
            "skipping 9908\n",
            "skipping 9909\n",
            "skipping 9910\n",
            "skipping 9911\n",
            "skipping 9912\n",
            "skipping 9913\n",
            "skipping 9914\n",
            "skipping 9915\n",
            "skipping 9916\n",
            "skipping 9917\n",
            "skipping 9919\n",
            "skipping 9920\n",
            "skipping 9921\n",
            "skipping 9922\n",
            "skipping 9923\n",
            "skipping 9924\n",
            "skipping 9925\n",
            "skipping 9926\n",
            "skipping 9927\n",
            "skipping 9928\n",
            "skipping 9929\n",
            "skipping 9930\n",
            "skipping 9931\n",
            "skipping 9932\n",
            "skipping 9933\n",
            "skipping 9934\n",
            "skipping 9935\n",
            "skipping 9936\n",
            "skipping 9937\n",
            "skipping 9938\n",
            "skipping 9939\n",
            "skipping 9940\n",
            "skipping 9941\n",
            "skipping 9942\n",
            "skipping 9943\n",
            "skipping 9944\n",
            "skipping 9945\n",
            "skipping 9946\n",
            "skipping 9947\n",
            "skipping 9948\n",
            "skipping 9949\n",
            "skipping 9950\n",
            "skipping 9951\n",
            "skipping 9952\n",
            "skipping 9953\n",
            "skipping 9954\n",
            "skipping 9955\n",
            "skipping 9956\n",
            "skipping 9957\n",
            "skipping 9958\n",
            "skipping 9959\n",
            "skipping 9960\n",
            "skipping 9961\n",
            "skipping 9962\n",
            "skipping 9963\n",
            "skipping 9964\n",
            "skipping 9965\n",
            "skipping 9966\n",
            "skipping 9967\n",
            "skipping 9968\n",
            "skipping 9969\n",
            "skipping 9970\n",
            "skipping 9972\n",
            "skipping 9973\n",
            "skipping 9974\n",
            "skipping 9975\n",
            "skipping 9976\n",
            "skipping 9977\n",
            "skipping 9978\n",
            "skipping 9979\n",
            "skipping 9980\n",
            "skipping 9981\n",
            "skipping 9982\n",
            "skipping 9983\n",
            "skipping 9984\n",
            "skipping 9985\n",
            "skipping 9986\n",
            "skipping 9987\n",
            "skipping 9988\n",
            "skipping 9989\n",
            "skipping 9990\n",
            "skipping 9991\n",
            "skipping 9992\n",
            "skipping 9993\n",
            "skipping 9994\n",
            "skipping 9995\n",
            "skipping 9996\n",
            "skipping 9997\n",
            "skipping 9998\n",
            "skipping 9999\n",
            "skipping 10000\n",
            "skipping 10001\n",
            "skipping 10002\n",
            "skipping 10003\n",
            "skipping 10004\n",
            "skipping 10005\n",
            "skipping 10006\n",
            "skipping 10007\n",
            "skipping 10008\n",
            "skipping 10009\n",
            "skipping 10010\n",
            "skipping 10011\n",
            "skipping 10012\n",
            "skipping 10013\n",
            "skipping 10014\n",
            "skipping 10015\n",
            "skipping 10017\n",
            "skipping 10018\n",
            "skipping 10019\n",
            "skipping 10020\n",
            "skipping 10021\n",
            "skipping 10022\n",
            "skipping 10023\n",
            "skipping 10024\n",
            "skipping 10025\n",
            "skipping 10026\n",
            "skipping 10027\n",
            "skipping 10028\n",
            "skipping 10029\n",
            "skipping 10030\n",
            "skipping 10031\n",
            "skipping 10032\n",
            "skipping 10033\n",
            "skipping 10034\n",
            "skipping 10035\n",
            "skipping 10036\n",
            "skipping 10037\n",
            "skipping 10038\n",
            "skipping 10039\n",
            "skipping 10040\n",
            "skipping 10041\n",
            "skipping 10042\n",
            "skipping 10043\n",
            "skipping 10044\n",
            "skipping 10045\n",
            "skipping 10046\n",
            "skipping 10047\n",
            "skipping 10048\n",
            "skipping 10049\n",
            "skipping 10050\n",
            "skipping 10051\n",
            "skipping 10052\n",
            "skipping 10053\n",
            "skipping 10054\n",
            "skipping 10055\n",
            "skipping 10056\n",
            "skipping 10057\n",
            "skipping 10058\n",
            "skipping 10059\n",
            "skipping 10060\n",
            "skipping 10061\n",
            "skipping 10062\n",
            "skipping 10063\n",
            "skipping 10064\n",
            "skipping 10065\n",
            "skipping 10066\n",
            "skipping 10067\n",
            "skipping 10068\n",
            "skipping 10069\n",
            "skipping 10071\n",
            "skipping 10072\n",
            "skipping 10073\n",
            "skipping 10074\n",
            "skipping 10075\n",
            "skipping 10076\n",
            "skipping 10077\n",
            "skipping 10078\n",
            "skipping 10079\n",
            "skipping 10080\n",
            "skipping 10081\n",
            "skipping 10082\n",
            "skipping 10083\n",
            "skipping 10084\n",
            "skipping 10085\n",
            "skipping 10086\n",
            "skipping 10087\n",
            "skipping 10088\n",
            "skipping 10089\n",
            "skipping 10090\n",
            "skipping 10091\n",
            "skipping 10092\n",
            "skipping 10093\n",
            "skipping 10094\n",
            "skipping 10095\n",
            "skipping 10096\n",
            "skipping 10097\n",
            "skipping 10098\n",
            "skipping 10099\n",
            "skipping 10100\n",
            "skipping 10101\n",
            "skipping 10102\n",
            "skipping 10104\n",
            "skipping 10105\n",
            "skipping 10106\n",
            "skipping 10107\n",
            "skipping 10108\n",
            "skipping 10109\n",
            "skipping 10110\n",
            "skipping 10111\n",
            "skipping 10112\n",
            "skipping 10113\n",
            "skipping 10114\n",
            "skipping 10115\n",
            "skipping 10116\n",
            "skipping 10117\n",
            "skipping 10118\n",
            "skipping 10119\n",
            "skipping 10120\n",
            "skipping 10121\n",
            "skipping 10122\n",
            "skipping 10123\n",
            "skipping 10124\n",
            "skipping 10126\n",
            "skipping 10127\n",
            "skipping 10128\n",
            "skipping 10129\n",
            "skipping 10130\n",
            "skipping 10131\n",
            "skipping 10132\n",
            "skipping 10133\n",
            "skipping 10134\n",
            "skipping 10135\n",
            "skipping 10136\n",
            "skipping 10137\n",
            "skipping 10138\n",
            "skipping 10139\n",
            "skipping 10140\n",
            "skipping 10141\n",
            "skipping 10142\n",
            "skipping 10143\n",
            "skipping 10144\n",
            "skipping 10145\n",
            "skipping 10146\n",
            "skipping 10147\n",
            "skipping 10148\n",
            "skipping 10149\n",
            "skipping 10150\n",
            "skipping 10151\n",
            "skipping 10152\n",
            "skipping 10153\n",
            "skipping 10154\n",
            "skipping 10155\n",
            "skipping 10156\n",
            "skipping 10157\n",
            "skipping 10158\n",
            "skipping 10159\n",
            "skipping 10160\n",
            "skipping 10161\n",
            "skipping 10162\n",
            "skipping 10163\n",
            "skipping 10164\n",
            "skipping 10165\n",
            "skipping 10166\n",
            "skipping 10167\n",
            "skipping 10168\n",
            "skipping 10170\n",
            "skipping 10171\n",
            "skipping 10172\n",
            "skipping 10173\n",
            "skipping 10174\n",
            "skipping 10175\n",
            "skipping 10176\n",
            "skipping 10177\n",
            "skipping 10178\n",
            "skipping 10179\n",
            "skipping 10180\n",
            "skipping 10181\n",
            "skipping 10182\n",
            "skipping 10183\n",
            "skipping 10184\n",
            "skipping 10185\n",
            "skipping 10186\n",
            "skipping 10187\n",
            "skipping 10188\n",
            "skipping 10189\n",
            "skipping 10190\n",
            "skipping 10191\n",
            "skipping 10192\n",
            "skipping 10193\n",
            "skipping 10195\n",
            "skipping 10196\n",
            "skipping 10197\n",
            "skipping 10198\n",
            "skipping 10199\n",
            "skipping 10200\n",
            "skipping 10201\n",
            "skipping 10202\n",
            "skipping 10203\n",
            "skipping 10204\n",
            "skipping 10205\n",
            "skipping 10206\n",
            "skipping 10207\n",
            "skipping 10208\n",
            "skipping 10209\n",
            "skipping 10210\n",
            "skipping 10211\n",
            "skipping 10212\n",
            "skipping 10213\n",
            "skipping 10214\n",
            "skipping 10215\n",
            "skipping 10216\n",
            "skipping 10217\n",
            "skipping 10218\n",
            "skipping 10219\n",
            "skipping 10220\n",
            "skipping 10221\n",
            "skipping 10222\n",
            "skipping 10223\n",
            "skipping 10224\n",
            "skipping 10225\n",
            "skipping 10227\n",
            "skipping 10228\n",
            "skipping 10229\n",
            "skipping 10230\n",
            "skipping 10231\n",
            "skipping 10232\n",
            "skipping 10233\n",
            "skipping 10234\n",
            "skipping 10236\n",
            "skipping 10237\n",
            "skipping 10238\n",
            "skipping 10239\n",
            "skipping 10240\n",
            "skipping 10241\n",
            "skipping 10242\n",
            "skipping 10243\n",
            "skipping 10244\n",
            "skipping 10245\n",
            "skipping 10246\n",
            "skipping 10247\n",
            "skipping 10248\n",
            "skipping 10249\n",
            "skipping 10250\n",
            "skipping 10251\n",
            "skipping 10252\n",
            "skipping 10253\n",
            "skipping 10254\n",
            "skipping 10255\n",
            "skipping 10256\n",
            "skipping 10257\n",
            "skipping 10258\n",
            "skipping 10259\n",
            "skipping 10260\n",
            "skipping 10261\n",
            "skipping 10262\n",
            "skipping 10263\n",
            "skipping 10264\n",
            "skipping 10265\n",
            "skipping 10266\n",
            "skipping 10267\n",
            "skipping 10268\n",
            "skipping 10269\n",
            "skipping 10270\n",
            "skipping 10271\n",
            "skipping 10272\n",
            "skipping 10273\n",
            "skipping 10274\n",
            "skipping 10275\n",
            "skipping 10277\n",
            "skipping 10278\n",
            "skipping 10279\n",
            "skipping 10280\n",
            "skipping 10281\n",
            "skipping 10282\n",
            "skipping 10283\n",
            "skipping 10284\n",
            "skipping 10285\n",
            "skipping 10286\n",
            "skipping 10287\n",
            "skipping 10288\n",
            "skipping 10289\n",
            "skipping 10290\n",
            "skipping 10291\n",
            "skipping 10292\n",
            "skipping 10293\n",
            "skipping 10294\n",
            "skipping 10295\n",
            "skipping 10296\n",
            "skipping 10297\n",
            "skipping 10298\n",
            "skipping 10299\n",
            "skipping 10300\n",
            "skipping 10301\n",
            "skipping 10302\n",
            "skipping 10303\n",
            "skipping 10304\n",
            "skipping 10305\n",
            "skipping 10306\n",
            "skipping 10307\n",
            "skipping 10308\n",
            "skipping 10309\n",
            "skipping 10310\n",
            "skipping 10311\n",
            "skipping 10312\n",
            "skipping 10313\n",
            "skipping 10314\n",
            "skipping 10315\n",
            "skipping 10316\n",
            "skipping 10317\n",
            "skipping 10318\n",
            "skipping 10319\n",
            "skipping 10320\n",
            "skipping 10321\n",
            "skipping 10322\n",
            "skipping 10323\n",
            "skipping 10324\n",
            "skipping 10325\n",
            "skipping 10326\n",
            "skipping 10327\n",
            "skipping 10328\n",
            "skipping 10329\n",
            "skipping 10330\n",
            "skipping 10331\n",
            "skipping 10332\n",
            "skipping 10333\n",
            "skipping 10334\n",
            "skipping 10335\n",
            "skipping 10336\n",
            "skipping 10338\n",
            "skipping 10339\n",
            "skipping 10340\n",
            "skipping 10341\n",
            "skipping 10342\n",
            "skipping 10343\n",
            "skipping 10344\n",
            "skipping 10345\n",
            "skipping 10346\n",
            "skipping 10347\n",
            "skipping 10348\n",
            "skipping 10350\n",
            "skipping 10351\n",
            "skipping 10352\n",
            "skipping 10353\n",
            "skipping 10354\n",
            "skipping 10355\n",
            "skipping 10356\n",
            "skipping 10357\n",
            "skipping 10358\n",
            "skipping 10359\n",
            "skipping 10360\n",
            "skipping 10361\n",
            "skipping 10362\n",
            "skipping 10363\n",
            "skipping 10364\n",
            "skipping 10366\n",
            "skipping 10367\n",
            "skipping 10368\n",
            "skipping 10369\n",
            "skipping 10370\n",
            "skipping 10371\n",
            "skipping 10372\n",
            "skipping 10373\n",
            "skipping 10374\n",
            "skipping 10375\n",
            "skipping 10376\n",
            "skipping 10377\n",
            "skipping 10378\n",
            "skipping 10379\n",
            "skipping 10380\n",
            "skipping 10381\n",
            "skipping 10382\n",
            "skipping 10383\n",
            "skipping 10384\n",
            "skipping 10385\n",
            "skipping 10386\n",
            "skipping 10387\n",
            "skipping 10388\n",
            "skipping 10389\n",
            "skipping 10390\n",
            "skipping 10391\n",
            "skipping 10392\n",
            "skipping 10393\n",
            "skipping 10394\n",
            "skipping 10395\n",
            "skipping 10396\n",
            "skipping 10397\n",
            "skipping 10398\n",
            "skipping 10399\n",
            "skipping 10400\n",
            "skipping 10401\n",
            "skipping 10402\n",
            "skipping 10403\n",
            "skipping 10404\n",
            "skipping 10405\n",
            "skipping 10406\n",
            "skipping 10407\n",
            "skipping 10408\n",
            "skipping 10409\n",
            "skipping 10410\n",
            "skipping 10411\n",
            "skipping 10412\n",
            "skipping 10413\n",
            "skipping 10414\n",
            "skipping 10415\n",
            "skipping 10416\n",
            "skipping 10417\n",
            "skipping 10418\n",
            "skipping 10419\n",
            "skipping 10420\n",
            "skipping 10421\n",
            "skipping 10422\n",
            "skipping 10423\n",
            "skipping 10424\n",
            "skipping 10425\n",
            "skipping 10426\n",
            "skipping 10427\n",
            "skipping 10428\n",
            "skipping 10429\n",
            "skipping 10430\n",
            "skipping 10431\n",
            "skipping 10432\n",
            "skipping 10433\n",
            "skipping 10434\n",
            "skipping 10435\n",
            "skipping 10436\n",
            "skipping 10437\n",
            "skipping 10438\n",
            "skipping 10439\n",
            "skipping 10440\n",
            "skipping 10441\n",
            "skipping 10442\n",
            "skipping 10443\n",
            "skipping 10444\n",
            "skipping 10445\n",
            "skipping 10446\n",
            "skipping 10447\n",
            "skipping 10448\n",
            "skipping 10449\n",
            "skipping 10450\n",
            "skipping 10451\n",
            "skipping 10452\n",
            "skipping 10453\n",
            "skipping 10454\n",
            "skipping 10455\n",
            "skipping 10456\n",
            "skipping 10457\n",
            "skipping 10458\n",
            "skipping 10459\n",
            "skipping 10460\n",
            "skipping 10461\n",
            "skipping 10462\n",
            "skipping 10463\n",
            "skipping 10464\n",
            "skipping 10465\n",
            "skipping 10466\n",
            "skipping 10467\n",
            "skipping 10468\n",
            "skipping 10469\n",
            "skipping 10470\n",
            "skipping 10471\n",
            "skipping 10472\n",
            "skipping 10474\n",
            "skipping 10475\n",
            "skipping 10476\n",
            "skipping 10477\n",
            "skipping 10478\n",
            "skipping 10479\n",
            "skipping 10480\n",
            "skipping 10481\n",
            "skipping 10482\n",
            "skipping 10483\n",
            "skipping 10484\n",
            "skipping 10485\n",
            "skipping 10486\n",
            "skipping 10487\n",
            "skipping 10488\n",
            "skipping 10489\n",
            "skipping 10490\n",
            "skipping 10491\n",
            "skipping 10492\n",
            "skipping 10493\n",
            "skipping 10494\n",
            "skipping 10495\n",
            "skipping 10496\n",
            "skipping 10497\n",
            "skipping 10498\n",
            "skipping 10499\n",
            "skipping 10500\n",
            "skipping 10501\n",
            "skipping 10502\n",
            "skipping 10503\n",
            "skipping 10504\n",
            "skipping 10505\n",
            "skipping 10506\n",
            "skipping 10507\n",
            "skipping 10508\n",
            "skipping 10509\n",
            "skipping 10510\n",
            "skipping 10511\n",
            "skipping 10512\n",
            "skipping 10513\n",
            "skipping 10514\n",
            "skipping 10515\n",
            "skipping 10516\n",
            "skipping 10518\n",
            "skipping 10519\n",
            "skipping 10520\n",
            "skipping 10521\n",
            "skipping 10522\n",
            "skipping 10523\n",
            "skipping 10524\n",
            "skipping 10525\n",
            "skipping 10526\n",
            "skipping 10527\n",
            "skipping 10528\n",
            "skipping 10529\n",
            "skipping 10531\n",
            "skipping 10532\n",
            "skipping 10533\n",
            "skipping 10534\n",
            "skipping 10535\n",
            "skipping 10536\n",
            "skipping 10537\n",
            "skipping 10538\n",
            "skipping 10539\n",
            "skipping 10540\n",
            "skipping 10541\n",
            "skipping 10542\n",
            "skipping 10543\n",
            "skipping 10544\n",
            "skipping 10545\n",
            "skipping 10546\n",
            "skipping 10547\n",
            "skipping 10548\n",
            "skipping 10549\n",
            "skipping 10550\n",
            "skipping 10551\n",
            "skipping 10552\n",
            "skipping 10553\n",
            "skipping 10554\n",
            "skipping 10555\n",
            "skipping 10556\n",
            "skipping 10557\n",
            "skipping 10558\n",
            "skipping 10559\n",
            "skipping 10560\n",
            "skipping 10561\n",
            "skipping 10562\n",
            "skipping 10563\n",
            "skipping 10564\n",
            "skipping 10565\n",
            "skipping 10566\n",
            "skipping 10567\n",
            "skipping 10568\n",
            "skipping 10569\n",
            "skipping 10570\n",
            "skipping 10571\n",
            "skipping 10572\n",
            "skipping 10573\n",
            "skipping 10574\n",
            "skipping 10576\n",
            "skipping 10577\n",
            "skipping 10578\n",
            "skipping 10579\n",
            "skipping 10580\n",
            "skipping 10581\n",
            "skipping 10582\n",
            "skipping 10583\n",
            "skipping 10584\n",
            "skipping 10585\n",
            "skipping 10586\n",
            "skipping 10587\n",
            "skipping 10588\n",
            "skipping 10589\n",
            "skipping 10590\n",
            "skipping 10591\n",
            "skipping 10592\n",
            "skipping 10593\n",
            "skipping 10594\n",
            "skipping 10595\n",
            "skipping 10596\n",
            "skipping 10597\n",
            "skipping 10598\n",
            "skipping 10599\n",
            "skipping 10600\n",
            "skipping 10601\n",
            "skipping 10602\n",
            "skipping 10603\n",
            "skipping 10605\n",
            "skipping 10606\n",
            "skipping 10607\n",
            "skipping 10608\n",
            "skipping 10609\n",
            "skipping 10610\n",
            "skipping 10611\n",
            "skipping 10612\n",
            "skipping 10613\n",
            "skipping 10614\n",
            "skipping 10615\n",
            "skipping 10616\n",
            "skipping 10617\n",
            "skipping 10618\n",
            "skipping 10619\n",
            "skipping 10620\n",
            "skipping 10621\n",
            "skipping 10622\n",
            "skipping 10623\n",
            "skipping 10624\n",
            "skipping 10626\n",
            "skipping 10627\n",
            "skipping 10628\n",
            "skipping 10629\n",
            "skipping 10630\n",
            "skipping 10631\n",
            "skipping 10632\n",
            "skipping 10633\n",
            "skipping 10634\n",
            "skipping 10635\n",
            "skipping 10636\n",
            "skipping 10637\n",
            "skipping 10638\n",
            "skipping 10639\n",
            "skipping 10640\n",
            "skipping 10641\n",
            "skipping 10642\n",
            "skipping 10643\n",
            "skipping 10644\n",
            "skipping 10645\n",
            "skipping 10646\n",
            "skipping 10647\n",
            "skipping 10648\n",
            "skipping 10649\n",
            "skipping 10650\n",
            "skipping 10651\n",
            "skipping 10652\n",
            "skipping 10653\n",
            "skipping 10654\n",
            "skipping 10655\n",
            "skipping 10656\n",
            "skipping 10657\n",
            "skipping 10658\n",
            "skipping 10659\n",
            "skipping 10660\n",
            "skipping 10661\n",
            "skipping 10662\n",
            "skipping 10663\n",
            "skipping 10664\n",
            "skipping 10665\n",
            "skipping 10666\n",
            "skipping 10667\n",
            "skipping 10668\n",
            "skipping 10669\n",
            "skipping 10670\n",
            "skipping 10671\n",
            "skipping 10672\n",
            "skipping 10673\n",
            "skipping 10674\n",
            "skipping 10675\n",
            "skipping 10676\n",
            "skipping 10678\n",
            "skipping 10679\n",
            "skipping 10680\n",
            "skipping 10681\n",
            "skipping 10682\n",
            "skipping 10683\n",
            "skipping 10684\n",
            "skipping 10685\n",
            "skipping 10686\n",
            "skipping 10687\n",
            "skipping 10688\n",
            "skipping 10689\n",
            "skipping 10690\n",
            "skipping 10691\n",
            "skipping 10692\n",
            "skipping 10693\n",
            "skipping 10694\n",
            "skipping 10695\n",
            "skipping 10696\n",
            "skipping 10697\n",
            "skipping 10698\n",
            "skipping 10699\n",
            "skipping 10700\n",
            "skipping 10701\n",
            "skipping 10702\n",
            "skipping 10703\n",
            "skipping 10704\n",
            "skipping 10705\n",
            "skipping 10706\n",
            "skipping 10707\n",
            "skipping 10708\n",
            "skipping 10709\n",
            "skipping 10710\n",
            "skipping 10711\n",
            "skipping 10712\n",
            "skipping 10713\n",
            "skipping 10714\n",
            "skipping 10715\n",
            "skipping 10716\n",
            "skipping 10717\n",
            "skipping 10718\n",
            "skipping 10719\n",
            "skipping 10720\n",
            "skipping 10721\n",
            "skipping 10722\n",
            "skipping 10723\n",
            "skipping 10724\n",
            "skipping 10725\n",
            "skipping 10726\n",
            "skipping 10727\n",
            "skipping 10728\n",
            "skipping 10729\n",
            "skipping 10730\n",
            "skipping 10731\n",
            "skipping 10732\n",
            "skipping 10733\n",
            "skipping 10734\n",
            "skipping 10735\n",
            "skipping 10736\n",
            "skipping 10737\n",
            "skipping 10738\n",
            "skipping 10739\n",
            "skipping 10740\n",
            "skipping 10741\n",
            "skipping 10742\n",
            "skipping 10743\n",
            "skipping 10744\n",
            "skipping 10745\n",
            "skipping 10746\n",
            "skipping 10747\n",
            "skipping 10748\n",
            "skipping 10749\n",
            "skipping 10750\n",
            "skipping 10751\n",
            "skipping 10752\n",
            "skipping 10753\n",
            "skipping 10754\n",
            "skipping 10755\n",
            "skipping 10756\n",
            "skipping 10757\n",
            "skipping 10758\n",
            "skipping 10760\n",
            "skipping 10761\n",
            "skipping 10762\n",
            "skipping 10763\n",
            "skipping 10764\n",
            "skipping 10765\n",
            "skipping 10766\n",
            "skipping 10767\n",
            "skipping 10768\n",
            "skipping 10769\n",
            "skipping 10770\n",
            "skipping 10771\n",
            "skipping 10772\n",
            "skipping 10773\n",
            "skipping 10774\n",
            "skipping 10776\n",
            "skipping 10777\n",
            "skipping 10778\n",
            "skipping 10779\n",
            "skipping 10780\n",
            "skipping 10781\n",
            "skipping 10782\n",
            "skipping 10783\n",
            "skipping 10784\n",
            "skipping 10785\n",
            "skipping 10786\n",
            "skipping 10787\n",
            "skipping 10788\n",
            "skipping 10789\n",
            "skipping 10790\n",
            "skipping 10791\n",
            "skipping 10792\n",
            "skipping 10793\n",
            "skipping 10794\n",
            "skipping 10795\n",
            "skipping 10796\n",
            "skipping 10797\n",
            "skipping 10798\n",
            "skipping 10799\n",
            "skipping 10800\n",
            "skipping 10801\n",
            "skipping 10802\n",
            "skipping 10803\n",
            "skipping 10804\n",
            "skipping 10805\n",
            "skipping 10806\n",
            "skipping 10807\n",
            "skipping 10808\n",
            "skipping 10809\n",
            "skipping 10810\n",
            "skipping 10811\n",
            "skipping 10812\n",
            "skipping 10813\n",
            "skipping 10814\n",
            "skipping 10815\n",
            "skipping 10816\n",
            "skipping 10817\n",
            "skipping 10818\n",
            "skipping 10819\n",
            "skipping 10820\n",
            "skipping 10821\n",
            "skipping 10822\n",
            "skipping 10823\n",
            "skipping 10824\n",
            "skipping 10825\n",
            "skipping 10826\n",
            "skipping 10827\n",
            "skipping 10828\n",
            "skipping 10829\n",
            "skipping 10830\n",
            "skipping 10831\n",
            "skipping 10832\n",
            "skipping 10833\n",
            "skipping 10834\n",
            "skipping 10835\n",
            "skipping 10837\n",
            "skipping 10838\n",
            "skipping 10839\n",
            "skipping 10840\n",
            "skipping 10841\n",
            "skipping 10842\n",
            "skipping 10843\n",
            "skipping 10844\n",
            "skipping 10845\n",
            "skipping 10846\n",
            "skipping 10847\n",
            "skipping 10848\n",
            "skipping 10849\n",
            "skipping 10850\n",
            "skipping 10851\n",
            "skipping 10852\n",
            "skipping 10853\n",
            "skipping 10854\n",
            "skipping 10855\n",
            "skipping 10856\n",
            "skipping 10857\n",
            "skipping 10858\n",
            "skipping 10859\n",
            "skipping 10860\n",
            "skipping 10862\n",
            "skipping 10863\n",
            "skipping 10864\n",
            "skipping 10865\n",
            "skipping 10866\n",
            "skipping 10867\n",
            "skipping 10868\n",
            "skipping 10869\n",
            "skipping 10870\n",
            "skipping 10871\n",
            "skipping 10872\n",
            "skipping 10873\n",
            "skipping 10874\n",
            "skipping 10875\n",
            "skipping 10876\n",
            "skipping 10877\n",
            "skipping 10878\n",
            "skipping 10879\n",
            "skipping 10880\n",
            "skipping 10881\n",
            "skipping 10882\n",
            "skipping 10883\n",
            "skipping 10884\n",
            "skipping 10885\n",
            "skipping 10886\n",
            "skipping 10887\n",
            "skipping 10888\n",
            "skipping 10889\n",
            "skipping 10890\n",
            "skipping 10891\n",
            "skipping 10892\n",
            "skipping 10893\n",
            "skipping 10894\n",
            "skipping 10895\n",
            "skipping 10896\n",
            "skipping 10897\n",
            "skipping 10898\n",
            "skipping 10899\n",
            "skipping 10900\n",
            "skipping 10901\n",
            "skipping 10902\n",
            "skipping 10903\n",
            "skipping 10905\n",
            "skipping 10906\n",
            "skipping 10907\n",
            "skipping 10908\n",
            "skipping 10909\n",
            "skipping 10910\n",
            "skipping 10911\n",
            "skipping 10912\n",
            "skipping 10913\n",
            "skipping 10914\n",
            "skipping 10915\n",
            "skipping 10916\n",
            "skipping 10917\n",
            "skipping 10918\n",
            "skipping 10919\n",
            "skipping 10920\n",
            "skipping 10921\n",
            "skipping 10922\n",
            "skipping 10923\n",
            "skipping 10924\n",
            "skipping 10925\n",
            "skipping 10926\n",
            "skipping 10927\n",
            "skipping 10928\n",
            "skipping 10929\n",
            "skipping 10930\n",
            "skipping 10931\n",
            "skipping 10932\n",
            "skipping 10933\n",
            "skipping 10934\n",
            "skipping 10935\n",
            "skipping 10936\n",
            "skipping 10937\n",
            "skipping 10938\n",
            "skipping 10939\n",
            "skipping 10940\n",
            "skipping 10941\n",
            "skipping 10942\n",
            "skipping 10943\n",
            "skipping 10944\n",
            "skipping 10945\n",
            "skipping 10946\n",
            "skipping 10947\n",
            "skipping 10948\n",
            "skipping 10949\n",
            "skipping 10950\n",
            "skipping 10951\n",
            "skipping 10952\n",
            "skipping 10953\n",
            "skipping 10954\n",
            "skipping 10955\n",
            "skipping 10957\n",
            "skipping 10958\n",
            "skipping 10959\n",
            "skipping 10960\n",
            "skipping 10961\n",
            "skipping 10962\n",
            "skipping 10963\n",
            "skipping 10964\n",
            "skipping 10965\n",
            "skipping 10966\n",
            "skipping 10967\n",
            "skipping 10968\n",
            "skipping 10969\n",
            "skipping 10970\n",
            "skipping 10971\n",
            "skipping 10972\n",
            "skipping 10973\n",
            "skipping 10974\n",
            "skipping 10975\n",
            "skipping 10976\n",
            "skipping 10977\n",
            "skipping 10978\n",
            "skipping 10979\n",
            "skipping 10980\n",
            "skipping 10981\n",
            "skipping 10982\n",
            "skipping 10983\n",
            "skipping 10984\n",
            "skipping 10985\n",
            "skipping 10986\n",
            "skipping 10987\n",
            "skipping 10988\n",
            "skipping 10989\n",
            "skipping 10990\n",
            "skipping 10991\n",
            "skipping 10992\n",
            "skipping 10993\n",
            "skipping 10994\n",
            "skipping 10995\n",
            "skipping 10996\n",
            "skipping 10997\n",
            "skipping 10998\n",
            "skipping 10999\n",
            "skipping 11000\n",
            "skipping 11001\n",
            "skipping 11002\n",
            "skipping 11003\n",
            "skipping 11004\n",
            "skipping 11005\n",
            "skipping 11006\n",
            "skipping 11007\n",
            "skipping 11008\n",
            "skipping 11009\n",
            "skipping 11011\n",
            "skipping 11012\n",
            "skipping 11013\n",
            "skipping 11014\n",
            "skipping 11015\n",
            "skipping 11016\n",
            "skipping 11017\n",
            "skipping 11018\n",
            "skipping 11019\n",
            "skipping 11020\n",
            "skipping 11022\n",
            "skipping 11023\n",
            "skipping 11024\n",
            "skipping 11025\n",
            "skipping 11026\n",
            "skipping 11027\n",
            "skipping 11028\n",
            "skipping 11029\n",
            "skipping 11030\n",
            "skipping 11031\n",
            "skipping 11032\n",
            "skipping 11033\n",
            "skipping 11034\n",
            "skipping 11035\n",
            "skipping 11036\n",
            "skipping 11037\n",
            "skipping 11038\n",
            "skipping 11039\n",
            "skipping 11040\n",
            "skipping 11041\n",
            "skipping 11042\n",
            "skipping 11043\n",
            "skipping 11044\n",
            "skipping 11045\n",
            "skipping 11046\n",
            "skipping 11047\n",
            "skipping 11048\n",
            "skipping 11049\n",
            "skipping 11050\n",
            "skipping 11051\n",
            "skipping 11052\n",
            "skipping 11053\n",
            "skipping 11054\n",
            "skipping 11055\n",
            "skipping 11056\n",
            "skipping 11057\n",
            "skipping 11058\n",
            "skipping 11059\n",
            "skipping 11060\n",
            "skipping 11061\n",
            "skipping 11062\n",
            "skipping 11063\n",
            "skipping 11064\n",
            "skipping 11065\n",
            "skipping 11066\n",
            "skipping 11067\n",
            "skipping 11069\n",
            "skipping 11070\n",
            "skipping 11071\n",
            "skipping 11072\n",
            "skipping 11073\n",
            "skipping 11074\n",
            "skipping 11075\n",
            "skipping 11076\n",
            "skipping 11077\n",
            "skipping 11078\n",
            "skipping 11079\n",
            "skipping 11080\n",
            "skipping 11081\n",
            "skipping 11082\n",
            "skipping 11083\n",
            "skipping 11084\n",
            "skipping 11085\n",
            "skipping 11086\n",
            "skipping 11087\n",
            "skipping 11088\n",
            "skipping 11089\n",
            "skipping 11090\n",
            "skipping 11091\n",
            "skipping 11092\n",
            "skipping 11093\n",
            "skipping 11094\n",
            "skipping 11095\n",
            "skipping 11096\n",
            "skipping 11097\n",
            "skipping 11098\n",
            "skipping 11099\n",
            "skipping 11100\n",
            "skipping 11101\n",
            "skipping 11102\n",
            "skipping 11103\n",
            "skipping 11104\n",
            "skipping 11105\n",
            "skipping 11106\n",
            "skipping 11107\n",
            "skipping 11109\n",
            "skipping 11110\n",
            "skipping 11111\n",
            "skipping 11112\n",
            "skipping 11113\n",
            "skipping 11114\n",
            "skipping 11115\n",
            "skipping 11116\n",
            "skipping 11117\n",
            "skipping 11118\n",
            "skipping 11119\n",
            "skipping 11120\n",
            "skipping 11121\n",
            "skipping 11122\n",
            "skipping 11123\n",
            "skipping 11124\n",
            "skipping 11125\n",
            "skipping 11126\n",
            "skipping 11127\n",
            "skipping 11128\n",
            "skipping 11129\n",
            "skipping 11130\n",
            "skipping 11131\n",
            "skipping 11132\n",
            "skipping 11133\n",
            "skipping 11134\n",
            "skipping 11135\n",
            "skipping 11136\n",
            "skipping 11138\n",
            "skipping 11139\n",
            "skipping 11140\n",
            "skipping 11141\n",
            "skipping 11142\n",
            "skipping 11143\n",
            "skipping 11144\n",
            "skipping 11145\n",
            "skipping 11146\n",
            "skipping 11147\n",
            "skipping 11148\n",
            "skipping 11149\n",
            "skipping 11150\n",
            "skipping 11151\n",
            "skipping 11152\n",
            "skipping 11153\n",
            "skipping 11154\n",
            "skipping 11155\n",
            "skipping 11156\n",
            "skipping 11157\n",
            "skipping 11158\n",
            "skipping 11159\n",
            "skipping 11160\n",
            "skipping 11161\n",
            "skipping 11162\n",
            "skipping 11163\n",
            "skipping 11164\n",
            "skipping 11165\n",
            "skipping 11166\n",
            "skipping 11167\n",
            "skipping 11168\n",
            "skipping 11169\n",
            "skipping 11170\n",
            "skipping 11171\n",
            "skipping 11172\n",
            "skipping 11173\n",
            "skipping 11174\n",
            "skipping 11175\n",
            "skipping 11176\n",
            "skipping 11177\n",
            "skipping 11178\n",
            "skipping 11179\n",
            "skipping 11180\n",
            "skipping 11182\n",
            "skipping 11183\n",
            "skipping 11184\n",
            "skipping 11185\n",
            "skipping 11186\n",
            "skipping 11187\n",
            "skipping 11188\n",
            "skipping 11189\n",
            "skipping 11190\n",
            "skipping 11191\n",
            "skipping 11192\n",
            "skipping 11193\n",
            "skipping 11194\n",
            "skipping 11195\n",
            "skipping 11196\n",
            "skipping 11197\n",
            "skipping 11198\n",
            "skipping 11199\n",
            "skipping 11200\n",
            "skipping 11201\n",
            "skipping 11202\n",
            "skipping 11203\n",
            "skipping 11204\n",
            "skipping 11205\n",
            "skipping 11206\n",
            "skipping 11207\n",
            "skipping 11208\n",
            "skipping 11209\n",
            "skipping 11210\n",
            "skipping 11211\n",
            "skipping 11212\n",
            "skipping 11213\n",
            "skipping 11214\n",
            "skipping 11215\n",
            "skipping 11216\n",
            "skipping 11217\n",
            "skipping 11218\n",
            "skipping 11219\n",
            "skipping 11220\n",
            "skipping 11221\n",
            "skipping 11222\n",
            "skipping 11223\n",
            "skipping 11224\n",
            "skipping 11225\n",
            "skipping 11226\n",
            "skipping 11228\n",
            "skipping 11229\n",
            "skipping 11230\n",
            "skipping 11231\n",
            "skipping 11232\n",
            "skipping 11234\n",
            "skipping 11235\n",
            "skipping 11236\n",
            "skipping 11237\n",
            "skipping 11238\n",
            "skipping 11239\n",
            "skipping 11240\n",
            "skipping 11241\n",
            "skipping 11242\n",
            "skipping 11243\n",
            "skipping 11244\n",
            "skipping 11245\n",
            "skipping 11246\n",
            "skipping 11247\n",
            "skipping 11248\n",
            "skipping 11249\n",
            "skipping 11250\n",
            "skipping 11251\n",
            "skipping 11252\n",
            "skipping 11253\n",
            "skipping 11254\n",
            "skipping 11255\n",
            "skipping 11256\n",
            "skipping 11257\n",
            "skipping 11258\n",
            "skipping 11259\n",
            "skipping 11260\n",
            "skipping 11261\n",
            "skipping 11262\n",
            "skipping 11263\n",
            "skipping 11264\n",
            "skipping 11265\n",
            "skipping 11266\n",
            "skipping 11267\n",
            "skipping 11268\n",
            "skipping 11269\n",
            "skipping 11270\n",
            "skipping 11271\n",
            "skipping 11272\n",
            "skipping 11273\n",
            "skipping 11274\n",
            "skipping 11275\n",
            "skipping 11276\n",
            "skipping 11278\n",
            "skipping 11279\n",
            "skipping 11280\n",
            "skipping 11281\n",
            "skipping 11282\n",
            "skipping 11283\n",
            "skipping 11284\n",
            "skipping 11285\n",
            "skipping 11286\n",
            "skipping 11288\n",
            "skipping 11289\n",
            "skipping 11290\n",
            "skipping 11291\n",
            "skipping 11292\n",
            "skipping 11293\n",
            "skipping 11294\n",
            "skipping 11295\n",
            "skipping 11296\n",
            "skipping 11297\n",
            "skipping 11298\n",
            "skipping 11299\n",
            "skipping 11301\n",
            "skipping 11302\n",
            "skipping 11303\n",
            "skipping 11304\n",
            "skipping 11305\n",
            "skipping 11306\n",
            "skipping 11307\n",
            "skipping 11308\n",
            "skipping 11309\n",
            "skipping 11310\n",
            "skipping 11311\n",
            "skipping 11312\n",
            "skipping 11313\n",
            "skipping 11314\n",
            "skipping 11315\n",
            "skipping 11316\n",
            "skipping 11317\n",
            "skipping 11319\n",
            "skipping 11320\n",
            "skipping 11321\n",
            "skipping 11322\n",
            "skipping 11323\n",
            "skipping 11324\n",
            "skipping 11325\n",
            "skipping 11326\n",
            "skipping 11327\n",
            "skipping 11328\n",
            "skipping 11329\n",
            "skipping 11330\n",
            "skipping 11331\n",
            "skipping 11332\n",
            "skipping 11333\n",
            "skipping 11334\n",
            "skipping 11335\n",
            "skipping 11336\n",
            "skipping 11337\n",
            "skipping 11338\n",
            "skipping 11339\n",
            "skipping 11340\n",
            "skipping 11341\n",
            "skipping 11342\n",
            "skipping 11343\n",
            "skipping 11344\n",
            "skipping 11346\n",
            "skipping 11347\n",
            "skipping 11348\n",
            "skipping 11349\n",
            "skipping 11350\n",
            "skipping 11351\n",
            "skipping 11352\n",
            "skipping 11353\n",
            "skipping 11354\n",
            "skipping 11355\n",
            "skipping 11356\n",
            "skipping 11357\n",
            "skipping 11358\n",
            "skipping 11359\n",
            "skipping 11361\n",
            "skipping 11362\n",
            "skipping 11363\n",
            "skipping 11364\n",
            "skipping 11365\n",
            "skipping 11366\n",
            "skipping 11367\n",
            "skipping 11368\n",
            "skipping 11369\n",
            "skipping 11370\n",
            "skipping 11371\n",
            "skipping 11372\n",
            "skipping 11373\n",
            "skipping 11374\n",
            "skipping 11375\n",
            "skipping 11376\n",
            "skipping 11377\n",
            "skipping 11378\n",
            "skipping 11379\n",
            "skipping 11380\n",
            "skipping 11381\n",
            "skipping 11382\n",
            "skipping 11383\n",
            "skipping 11384\n",
            "skipping 11385\n",
            "skipping 11386\n",
            "skipping 11387\n",
            "skipping 11388\n",
            "skipping 11389\n",
            "skipping 11390\n",
            "skipping 11391\n",
            "skipping 11392\n",
            "skipping 11393\n",
            "skipping 11394\n",
            "skipping 11395\n",
            "skipping 11396\n",
            "skipping 11397\n",
            "skipping 11398\n",
            "skipping 11399\n",
            "skipping 11400\n",
            "skipping 11401\n",
            "skipping 11402\n",
            "skipping 11403\n",
            "skipping 11404\n",
            "skipping 11405\n",
            "skipping 11406\n",
            "skipping 11407\n",
            "skipping 11408\n",
            "skipping 11409\n",
            "skipping 11410\n",
            "skipping 11411\n",
            "skipping 11412\n",
            "skipping 11413\n",
            "skipping 11414\n",
            "skipping 11415\n",
            "skipping 11416\n",
            "skipping 11417\n",
            "skipping 11418\n",
            "skipping 11419\n",
            "skipping 11420\n",
            "skipping 11421\n",
            "skipping 11422\n",
            "skipping 11423\n",
            "skipping 11424\n",
            "skipping 11425\n",
            "skipping 11427\n",
            "skipping 11428\n",
            "skipping 11429\n",
            "skipping 11430\n",
            "skipping 11431\n",
            "skipping 11432\n",
            "skipping 11433\n",
            "skipping 11434\n",
            "skipping 11435\n",
            "skipping 11436\n",
            "skipping 11437\n",
            "skipping 11438\n",
            "skipping 11439\n",
            "skipping 11440\n",
            "skipping 11441\n",
            "skipping 11442\n",
            "skipping 11443\n",
            "skipping 11444\n",
            "skipping 11445\n",
            "skipping 11446\n",
            "skipping 11447\n",
            "skipping 11448\n",
            "skipping 11449\n",
            "skipping 11450\n",
            "skipping 11451\n",
            "skipping 11452\n",
            "skipping 11454\n",
            "skipping 11455\n",
            "skipping 11456\n",
            "skipping 11457\n",
            "skipping 11458\n",
            "skipping 11459\n",
            "skipping 11460\n",
            "skipping 11461\n",
            "skipping 11462\n",
            "skipping 11463\n",
            "skipping 11464\n",
            "skipping 11465\n",
            "skipping 11466\n",
            "skipping 11467\n",
            "skipping 11468\n",
            "skipping 11469\n",
            "skipping 11470\n",
            "skipping 11471\n",
            "skipping 11472\n",
            "skipping 11473\n",
            "skipping 11474\n",
            "skipping 11475\n",
            "skipping 11476\n",
            "skipping 11477\n",
            "skipping 11478\n",
            "skipping 11479\n",
            "skipping 11480\n",
            "skipping 11481\n",
            "skipping 11482\n",
            "skipping 11483\n",
            "skipping 11484\n",
            "skipping 11485\n",
            "skipping 11486\n",
            "skipping 11487\n",
            "skipping 11488\n",
            "skipping 11489\n",
            "skipping 11490\n",
            "skipping 11491\n",
            "skipping 11492\n",
            "skipping 11493\n",
            "skipping 11494\n",
            "skipping 11496\n",
            "skipping 11497\n",
            "skipping 11498\n",
            "skipping 11499\n",
            "skipping 11500\n",
            "skipping 11501\n",
            "skipping 11502\n",
            "skipping 11503\n",
            "skipping 11504\n",
            "skipping 11505\n",
            "skipping 11506\n",
            "skipping 11507\n",
            "skipping 11509\n",
            "skipping 11510\n",
            "skipping 11511\n",
            "skipping 11512\n",
            "skipping 11513\n",
            "skipping 11514\n",
            "skipping 11515\n",
            "skipping 11516\n",
            "skipping 11517\n",
            "skipping 11518\n",
            "skipping 11519\n",
            "skipping 11520\n",
            "skipping 11521\n",
            "skipping 11522\n",
            "skipping 11523\n",
            "skipping 11524\n",
            "skipping 11525\n",
            "skipping 11526\n",
            "skipping 11527\n",
            "skipping 11528\n",
            "skipping 11529\n",
            "skipping 11530\n",
            "skipping 11531\n",
            "skipping 11532\n",
            "skipping 11533\n",
            "skipping 11534\n",
            "skipping 11535\n",
            "skipping 11536\n",
            "skipping 11537\n",
            "skipping 11538\n",
            "skipping 11539\n",
            "skipping 11540\n",
            "skipping 11541\n",
            "skipping 11542\n",
            "skipping 11543\n",
            "skipping 11544\n",
            "skipping 11545\n",
            "skipping 11546\n",
            "skipping 11547\n",
            "skipping 11548\n",
            "skipping 11549\n",
            "skipping 11550\n",
            "skipping 11551\n",
            "skipping 11552\n",
            "skipping 11553\n",
            "skipping 11554\n",
            "skipping 11555\n",
            "skipping 11556\n",
            "skipping 11557\n",
            "skipping 11558\n",
            "skipping 11559\n",
            "skipping 11560\n",
            "skipping 11561\n",
            "skipping 11562\n",
            "skipping 11563\n",
            "skipping 11564\n",
            "skipping 11565\n",
            "skipping 11566\n",
            "skipping 11567\n",
            "skipping 11568\n",
            "skipping 11569\n",
            "skipping 11570\n",
            "skipping 11571\n",
            "skipping 11572\n",
            "skipping 11573\n",
            "skipping 11574\n",
            "skipping 11575\n",
            "skipping 11576\n",
            "skipping 11577\n",
            "skipping 11578\n",
            "skipping 11579\n",
            "skipping 11580\n",
            "skipping 11581\n",
            "skipping 11582\n",
            "skipping 11583\n",
            "skipping 11584\n",
            "skipping 11585\n",
            "skipping 11587\n",
            "skipping 11588\n",
            "skipping 11589\n",
            "skipping 11590\n",
            "skipping 11591\n",
            "skipping 11592\n",
            "skipping 11593\n",
            "skipping 11594\n",
            "skipping 11595\n",
            "skipping 11596\n",
            "skipping 11597\n",
            "skipping 11598\n",
            "skipping 11599\n",
            "skipping 11600\n",
            "skipping 11601\n",
            "skipping 11602\n",
            "skipping 11603\n",
            "skipping 11604\n",
            "skipping 11605\n",
            "skipping 11606\n",
            "skipping 11607\n",
            "skipping 11608\n",
            "skipping 11609\n",
            "skipping 11610\n",
            "skipping 11611\n",
            "skipping 11612\n",
            "skipping 11613\n",
            "skipping 11614\n",
            "skipping 11615\n",
            "skipping 11616\n",
            "skipping 11617\n",
            "skipping 11618\n",
            "skipping 11619\n",
            "skipping 11620\n",
            "skipping 11621\n",
            "skipping 11622\n",
            "skipping 11623\n",
            "skipping 11624\n",
            "skipping 11625\n",
            "skipping 11626\n",
            "skipping 11627\n",
            "skipping 11628\n",
            "skipping 11629\n",
            "skipping 11630\n",
            "skipping 11631\n",
            "skipping 11632\n",
            "skipping 11633\n",
            "skipping 11634\n",
            "skipping 11635\n",
            "skipping 11636\n",
            "skipping 11637\n",
            "skipping 11638\n",
            "skipping 11639\n",
            "skipping 11640\n",
            "skipping 11641\n",
            "skipping 11642\n",
            "skipping 11643\n",
            "skipping 11644\n",
            "skipping 11646\n",
            "skipping 11647\n",
            "skipping 11648\n",
            "skipping 11649\n",
            "skipping 11650\n",
            "skipping 11651\n",
            "skipping 11652\n",
            "skipping 11653\n",
            "skipping 11654\n",
            "skipping 11655\n",
            "skipping 11656\n",
            "skipping 11657\n",
            "skipping 11658\n",
            "skipping 11659\n",
            "skipping 11660\n",
            "skipping 11661\n",
            "skipping 11662\n",
            "skipping 11663\n",
            "skipping 11664\n",
            "skipping 11665\n",
            "skipping 11666\n",
            "skipping 11667\n",
            "skipping 11668\n",
            "skipping 11669\n",
            "skipping 11670\n",
            "skipping 11671\n",
            "skipping 11672\n",
            "skipping 11673\n",
            "skipping 11674\n",
            "skipping 11675\n",
            "skipping 11676\n",
            "skipping 11677\n",
            "skipping 11678\n",
            "skipping 11679\n",
            "skipping 11680\n",
            "skipping 11681\n",
            "skipping 11682\n",
            "skipping 11683\n",
            "skipping 11684\n",
            "skipping 11685\n",
            "skipping 11686\n",
            "skipping 11687\n",
            "skipping 11688\n",
            "skipping 11689\n",
            "skipping 11690\n",
            "skipping 11691\n",
            "skipping 11692\n",
            "skipping 11693\n",
            "skipping 11694\n",
            "skipping 11695\n",
            "skipping 11696\n",
            "skipping 11697\n",
            "skipping 11698\n",
            "skipping 11699\n",
            "skipping 11700\n",
            "skipping 11701\n",
            "skipping 11702\n",
            "skipping 11703\n",
            "skipping 11704\n",
            "skipping 11705\n",
            "skipping 11706\n",
            "skipping 11707\n",
            "skipping 11708\n",
            "skipping 11709\n",
            "skipping 11710\n",
            "skipping 11711\n",
            "skipping 11713\n",
            "skipping 11714\n",
            "skipping 11715\n",
            "skipping 11716\n",
            "skipping 11717\n",
            "skipping 11718\n",
            "skipping 11719\n",
            "skipping 11720\n",
            "skipping 11721\n",
            "skipping 11722\n",
            "skipping 11723\n",
            "skipping 11724\n",
            "skipping 11725\n",
            "skipping 11726\n",
            "skipping 11727\n",
            "skipping 11728\n",
            "skipping 11729\n",
            "skipping 11730\n",
            "skipping 11731\n",
            "skipping 11732\n",
            "skipping 11733\n",
            "skipping 11734\n",
            "skipping 11735\n",
            "skipping 11736\n",
            "skipping 11737\n",
            "skipping 11738\n",
            "skipping 11739\n",
            "skipping 11740\n",
            "skipping 11741\n",
            "skipping 11742\n",
            "skipping 11743\n",
            "skipping 11744\n",
            "skipping 11746\n",
            "skipping 11747\n",
            "skipping 11748\n",
            "skipping 11749\n",
            "skipping 11750\n",
            "skipping 11751\n",
            "skipping 11752\n",
            "skipping 11753\n",
            "skipping 11754\n",
            "skipping 11755\n",
            "skipping 11756\n",
            "skipping 11757\n",
            "skipping 11758\n",
            "skipping 11759\n",
            "skipping 11760\n",
            "skipping 11761\n",
            "skipping 11762\n",
            "skipping 11763\n",
            "skipping 11764\n",
            "skipping 11765\n",
            "skipping 11766\n",
            "skipping 11767\n",
            "skipping 11768\n",
            "skipping 11769\n",
            "skipping 11770\n",
            "skipping 11771\n",
            "skipping 11772\n",
            "skipping 11773\n",
            "skipping 11774\n",
            "skipping 11775\n",
            "skipping 11776\n",
            "skipping 11778\n",
            "skipping 11779\n",
            "skipping 11780\n",
            "skipping 11781\n",
            "skipping 11782\n",
            "skipping 11783\n",
            "skipping 11784\n",
            "skipping 11785\n",
            "skipping 11786\n",
            "skipping 11787\n",
            "skipping 11788\n",
            "skipping 11789\n",
            "skipping 11790\n",
            "skipping 11791\n",
            "skipping 11792\n",
            "skipping 11793\n",
            "skipping 11794\n",
            "skipping 11795\n",
            "skipping 11796\n",
            "skipping 11797\n",
            "skipping 11798\n",
            "skipping 11799\n",
            "skipping 11800\n",
            "skipping 11801\n",
            "skipping 11802\n",
            "skipping 11803\n",
            "skipping 11804\n",
            "skipping 11805\n",
            "skipping 11806\n",
            "skipping 11807\n",
            "skipping 11808\n",
            "skipping 11809\n",
            "skipping 11810\n",
            "skipping 11811\n",
            "skipping 11812\n",
            "skipping 11813\n",
            "skipping 11814\n",
            "skipping 11815\n",
            "skipping 11816\n",
            "skipping 11817\n",
            "skipping 11818\n",
            "skipping 11819\n",
            "skipping 11820\n",
            "skipping 11821\n",
            "skipping 11822\n",
            "skipping 11823\n",
            "skipping 11824\n",
            "skipping 11825\n",
            "skipping 11826\n",
            "skipping 11827\n",
            "skipping 11828\n",
            "skipping 11829\n",
            "skipping 11830\n",
            "skipping 11831\n",
            "skipping 11832\n",
            "skipping 11833\n",
            "skipping 11834\n",
            "skipping 11835\n",
            "skipping 11836\n",
            "skipping 11837\n",
            "skipping 11838\n",
            "skipping 11839\n",
            "skipping 11840\n",
            "skipping 11841\n",
            "skipping 11842\n",
            "skipping 11843\n",
            "skipping 11844\n",
            "skipping 11845\n",
            "skipping 11846\n",
            "skipping 11847\n",
            "skipping 11848\n",
            "skipping 11849\n",
            "skipping 11850\n",
            "skipping 11851\n",
            "skipping 11852\n",
            "skipping 11853\n",
            "skipping 11854\n",
            "skipping 11855\n",
            "skipping 11856\n",
            "skipping 11857\n",
            "skipping 11858\n",
            "skipping 11859\n",
            "skipping 11860\n",
            "skipping 11861\n",
            "skipping 11862\n",
            "skipping 11863\n",
            "skipping 11864\n",
            "skipping 11865\n",
            "skipping 11866\n",
            "skipping 11867\n",
            "skipping 11868\n",
            "skipping 11869\n",
            "skipping 11870\n",
            "skipping 11871\n",
            "skipping 11872\n",
            "skipping 11873\n",
            "skipping 11874\n",
            "skipping 11875\n",
            "skipping 11876\n",
            "skipping 11877\n",
            "skipping 11878\n",
            "skipping 11879\n",
            "skipping 11881\n",
            "skipping 11882\n",
            "skipping 11883\n",
            "skipping 11884\n",
            "skipping 11885\n",
            "skipping 11886\n",
            "skipping 11887\n",
            "skipping 11888\n",
            "skipping 11889\n",
            "skipping 11890\n",
            "skipping 11891\n",
            "skipping 11892\n",
            "skipping 11893\n",
            "skipping 11894\n",
            "skipping 11895\n",
            "skipping 11896\n",
            "skipping 11897\n",
            "skipping 11898\n",
            "skipping 11899\n",
            "skipping 11900\n",
            "skipping 11901\n",
            "skipping 11902\n",
            "skipping 11903\n",
            "skipping 11904\n",
            "skipping 11905\n",
            "skipping 11906\n",
            "skipping 11907\n",
            "skipping 11908\n",
            "skipping 11909\n",
            "skipping 11910\n",
            "skipping 11911\n",
            "skipping 11912\n",
            "skipping 11913\n",
            "skipping 11914\n",
            "skipping 11915\n",
            "skipping 11916\n",
            "skipping 11917\n",
            "skipping 11919\n",
            "skipping 11920\n",
            "skipping 11921\n",
            "skipping 11922\n",
            "skipping 11923\n",
            "skipping 11924\n",
            "skipping 11925\n",
            "skipping 11926\n",
            "skipping 11927\n",
            "skipping 11928\n",
            "skipping 11929\n",
            "skipping 11930\n",
            "skipping 11931\n",
            "skipping 11932\n",
            "skipping 11933\n",
            "skipping 11934\n",
            "skipping 11935\n",
            "skipping 11936\n",
            "skipping 11937\n",
            "skipping 11938\n",
            "skipping 11939\n",
            "skipping 11940\n",
            "skipping 11941\n",
            "skipping 11942\n",
            "skipping 11943\n",
            "skipping 11944\n",
            "skipping 11945\n",
            "skipping 11946\n",
            "skipping 11947\n",
            "skipping 11948\n",
            "skipping 11949\n",
            "skipping 11950\n",
            "skipping 11951\n",
            "skipping 11952\n",
            "skipping 11953\n",
            "skipping 11954\n",
            "skipping 11955\n",
            "skipping 11956\n",
            "skipping 11957\n",
            "skipping 11958\n",
            "skipping 11959\n",
            "skipping 11960\n",
            "skipping 11961\n",
            "skipping 11962\n",
            "skipping 11963\n",
            "skipping 11964\n",
            "skipping 11965\n",
            "skipping 11966\n",
            "skipping 11967\n",
            "skipping 11968\n",
            "skipping 11969\n",
            "skipping 11970\n",
            "skipping 11971\n",
            "skipping 11972\n",
            "skipping 11973\n",
            "skipping 11974\n",
            "skipping 11975\n",
            "skipping 11976\n",
            "skipping 11977\n",
            "skipping 11978\n",
            "skipping 11979\n",
            "skipping 11980\n",
            "skipping 11981\n",
            "skipping 11982\n",
            "skipping 11983\n",
            "skipping 11984\n",
            "skipping 11985\n",
            "skipping 11986\n",
            "skipping 11987\n",
            "skipping 11988\n",
            "skipping 11989\n",
            "skipping 11990\n",
            "skipping 11991\n",
            "skipping 11992\n",
            "skipping 11993\n",
            "skipping 11994\n",
            "skipping 11995\n",
            "skipping 11996\n",
            "skipping 11997\n",
            "skipping 11998\n",
            "skipping 11999\n",
            "skipping 12000\n",
            "skipping 12001\n",
            "skipping 12002\n",
            "skipping 12003\n",
            "skipping 12004\n",
            "skipping 12005\n",
            "skipping 12006\n",
            "skipping 12007\n",
            "skipping 12008\n",
            "skipping 12009\n",
            "skipping 12010\n",
            "skipping 12011\n",
            "skipping 12012\n",
            "skipping 12013\n",
            "skipping 12014\n",
            "skipping 12015\n",
            "skipping 12016\n",
            "skipping 12017\n",
            "skipping 12018\n",
            "skipping 12019\n",
            "skipping 12020\n",
            "skipping 12021\n",
            "skipping 12022\n",
            "skipping 12023\n",
            "skipping 12024\n",
            "skipping 12025\n",
            "skipping 12026\n",
            "skipping 12027\n",
            "skipping 12028\n",
            "skipping 12029\n",
            "skipping 12030\n",
            "skipping 12031\n",
            "skipping 12032\n",
            "skipping 12033\n",
            "skipping 12034\n",
            "skipping 12035\n",
            "skipping 12036\n",
            "skipping 12037\n",
            "skipping 12038\n",
            "skipping 12039\n",
            "skipping 12040\n",
            "skipping 12041\n",
            "skipping 12042\n",
            "skipping 12043\n",
            "skipping 12044\n",
            "skipping 12045\n",
            "skipping 12046\n",
            "skipping 12047\n",
            "skipping 12048\n",
            "skipping 12049\n",
            "skipping 12050\n",
            "skipping 12051\n",
            "skipping 12052\n",
            "skipping 12053\n",
            "skipping 12054\n",
            "skipping 12055\n",
            "skipping 12056\n",
            "skipping 12057\n",
            "skipping 12058\n",
            "skipping 12059\n",
            "skipping 12060\n",
            "skipping 12061\n",
            "skipping 12062\n",
            "skipping 12063\n",
            "skipping 12065\n",
            "skipping 12066\n",
            "skipping 12067\n",
            "skipping 12068\n",
            "skipping 12069\n",
            "skipping 12070\n",
            "skipping 12071\n",
            "skipping 12072\n",
            "skipping 12073\n",
            "skipping 12074\n",
            "skipping 12075\n",
            "skipping 12076\n",
            "skipping 12077\n",
            "skipping 12078\n",
            "skipping 12079\n",
            "skipping 12080\n",
            "skipping 12081\n",
            "skipping 12082\n",
            "skipping 12083\n",
            "skipping 12084\n",
            "skipping 12085\n",
            "skipping 12086\n",
            "skipping 12087\n",
            "skipping 12088\n",
            "skipping 12089\n",
            "skipping 12090\n",
            "skipping 12091\n",
            "skipping 12092\n",
            "skipping 12093\n",
            "skipping 12094\n",
            "skipping 12095\n",
            "skipping 12096\n",
            "skipping 12097\n",
            "skipping 12098\n",
            "skipping 12099\n",
            "skipping 12100\n",
            "skipping 12101\n",
            "skipping 12102\n",
            "skipping 12103\n",
            "skipping 12104\n",
            "skipping 12105\n",
            "skipping 12106\n",
            "skipping 12107\n",
            "skipping 12108\n",
            "skipping 12109\n",
            "skipping 12110\n",
            "skipping 12111\n",
            "skipping 12112\n",
            "skipping 12113\n",
            "skipping 12114\n",
            "skipping 12115\n",
            "skipping 12116\n",
            "skipping 12117\n",
            "skipping 12118\n",
            "skipping 12119\n",
            "skipping 12120\n",
            "skipping 12121\n",
            "skipping 12122\n",
            "skipping 12123\n",
            "skipping 12124\n",
            "skipping 12125\n",
            "skipping 12126\n",
            "skipping 12127\n",
            "skipping 12128\n",
            "skipping 12129\n",
            "skipping 12130\n",
            "skipping 12131\n",
            "skipping 12132\n",
            "skipping 12133\n",
            "skipping 12134\n",
            "skipping 12135\n",
            "skipping 12136\n",
            "skipping 12137\n",
            "skipping 12138\n",
            "skipping 12139\n",
            "skipping 12140\n",
            "skipping 12141\n",
            "skipping 12142\n",
            "skipping 12143\n",
            "skipping 12144\n",
            "skipping 12145\n",
            "skipping 12146\n",
            "skipping 12147\n",
            "skipping 12148\n",
            "skipping 12149\n",
            "skipping 12150\n",
            "skipping 12151\n",
            "skipping 12152\n",
            "skipping 12153\n",
            "skipping 12154\n",
            "skipping 12155\n",
            "skipping 12156\n",
            "skipping 12157\n",
            "skipping 12158\n",
            "skipping 12159\n",
            "skipping 12160\n",
            "skipping 12161\n",
            "skipping 12162\n",
            "skipping 12163\n",
            "skipping 12164\n",
            "skipping 12165\n",
            "skipping 12166\n",
            "skipping 12167\n",
            "skipping 12169\n",
            "skipping 12170\n",
            "skipping 12171\n",
            "skipping 12173\n",
            "skipping 12174\n",
            "skipping 12175\n",
            "skipping 12176\n",
            "skipping 12177\n",
            "skipping 12178\n",
            "skipping 12179\n",
            "skipping 12180\n",
            "skipping 12181\n",
            "skipping 12182\n",
            "skipping 12183\n",
            "skipping 12184\n",
            "skipping 12185\n",
            "skipping 12186\n",
            "skipping 12187\n",
            "skipping 12188\n",
            "skipping 12189\n",
            "skipping 12190\n",
            "skipping 12191\n",
            "skipping 12192\n",
            "skipping 12193\n",
            "skipping 12194\n",
            "skipping 12195\n",
            "skipping 12196\n",
            "skipping 12197\n",
            "skipping 12198\n",
            "skipping 12199\n",
            "skipping 12200\n",
            "skipping 12201\n",
            "skipping 12202\n",
            "skipping 12203\n",
            "skipping 12204\n",
            "skipping 12205\n",
            "skipping 12206\n",
            "skipping 12207\n",
            "skipping 12208\n",
            "skipping 12209\n",
            "skipping 12210\n",
            "skipping 12211\n",
            "skipping 12212\n",
            "skipping 12213\n",
            "skipping 12214\n",
            "skipping 12215\n",
            "skipping 12216\n",
            "skipping 12217\n",
            "skipping 12218\n",
            "skipping 12219\n",
            "skipping 12220\n",
            "skipping 12221\n",
            "skipping 12222\n",
            "skipping 12223\n",
            "skipping 12224\n",
            "skipping 12225\n",
            "skipping 12226\n",
            "skipping 12227\n",
            "skipping 12228\n",
            "skipping 12229\n",
            "skipping 12230\n",
            "skipping 12231\n",
            "skipping 12232\n",
            "skipping 12233\n",
            "skipping 12234\n",
            "skipping 12235\n",
            "skipping 12236\n",
            "skipping 12238\n",
            "skipping 12239\n",
            "skipping 12240\n",
            "skipping 12241\n",
            "skipping 12242\n",
            "skipping 12243\n",
            "skipping 12244\n",
            "skipping 12245\n",
            "skipping 12246\n",
            "skipping 12247\n",
            "skipping 12248\n",
            "skipping 12249\n",
            "skipping 12250\n",
            "skipping 12251\n",
            "skipping 12253\n",
            "skipping 12254\n",
            "skipping 12255\n",
            "skipping 12256\n",
            "skipping 12257\n",
            "skipping 12258\n",
            "skipping 12259\n",
            "skipping 12260\n",
            "skipping 12261\n",
            "skipping 12262\n",
            "skipping 12263\n",
            "skipping 12264\n",
            "skipping 12265\n",
            "skipping 12266\n",
            "skipping 12267\n",
            "skipping 12268\n",
            "skipping 12269\n",
            "skipping 12270\n",
            "skipping 12271\n",
            "skipping 12272\n",
            "skipping 12273\n",
            "skipping 12274\n",
            "skipping 12275\n",
            "skipping 12276\n",
            "skipping 12277\n",
            "skipping 12278\n",
            "skipping 12279\n",
            "skipping 12280\n",
            "skipping 12281\n",
            "skipping 12282\n",
            "skipping 12283\n",
            "skipping 12284\n",
            "skipping 12285\n",
            "skipping 12286\n",
            "skipping 12287\n",
            "skipping 12288\n",
            "skipping 12289\n",
            "skipping 12290\n",
            "skipping 12291\n",
            "skipping 12292\n",
            "skipping 12293\n",
            "skipping 12295\n",
            "skipping 12296\n",
            "skipping 12297\n",
            "skipping 12298\n",
            "skipping 12299\n",
            "skipping 12300\n",
            "skipping 12301\n",
            "skipping 12302\n",
            "skipping 12303\n",
            "skipping 12304\n",
            "skipping 12305\n",
            "skipping 12306\n",
            "skipping 12307\n",
            "skipping 12308\n",
            "skipping 12309\n",
            "skipping 12310\n",
            "skipping 12311\n",
            "skipping 12312\n",
            "skipping 12313\n",
            "skipping 12314\n",
            "skipping 12315\n",
            "skipping 12316\n",
            "skipping 12317\n",
            "skipping 12318\n",
            "skipping 12319\n",
            "skipping 12321\n",
            "skipping 12322\n",
            "skipping 12323\n",
            "skipping 12324\n",
            "skipping 12325\n",
            "skipping 12326\n",
            "skipping 12327\n",
            "skipping 12328\n",
            "skipping 12329\n",
            "skipping 12330\n",
            "skipping 12331\n",
            "skipping 12332\n",
            "skipping 12333\n",
            "skipping 12334\n",
            "skipping 12335\n",
            "skipping 12336\n",
            "skipping 12337\n",
            "skipping 12338\n",
            "skipping 12339\n",
            "skipping 12340\n",
            "skipping 12341\n",
            "skipping 12342\n",
            "skipping 12343\n",
            "skipping 12344\n",
            "skipping 12345\n",
            "skipping 12346\n",
            "skipping 12348\n",
            "skipping 12349\n",
            "skipping 12350\n",
            "skipping 12351\n",
            "skipping 12352\n",
            "skipping 12353\n",
            "skipping 12354\n",
            "skipping 12355\n",
            "skipping 12356\n",
            "skipping 12357\n",
            "skipping 12358\n",
            "skipping 12359\n",
            "skipping 12360\n",
            "skipping 12361\n",
            "skipping 12362\n",
            "skipping 12363\n",
            "skipping 12364\n",
            "skipping 12365\n",
            "skipping 12366\n",
            "skipping 12368\n",
            "skipping 12369\n",
            "skipping 12370\n",
            "skipping 12371\n",
            "skipping 12372\n",
            "skipping 12373\n",
            "skipping 12374\n",
            "skipping 12375\n",
            "skipping 12376\n",
            "skipping 12377\n",
            "skipping 12378\n",
            "skipping 12379\n",
            "skipping 12380\n",
            "skipping 12381\n",
            "skipping 12382\n",
            "skipping 12383\n",
            "skipping 12384\n",
            "skipping 12385\n",
            "skipping 12386\n",
            "skipping 12387\n",
            "skipping 12388\n",
            "skipping 12389\n",
            "skipping 12390\n",
            "skipping 12391\n",
            "skipping 12392\n",
            "skipping 12393\n",
            "skipping 12394\n",
            "skipping 12395\n",
            "skipping 12396\n",
            "skipping 12397\n",
            "skipping 12398\n",
            "skipping 12399\n",
            "skipping 12400\n",
            "skipping 12401\n",
            "skipping 12402\n",
            "skipping 12403\n",
            "skipping 12404\n",
            "skipping 12405\n",
            "skipping 12406\n",
            "skipping 12407\n",
            "skipping 12408\n",
            "skipping 12409\n",
            "skipping 12410\n",
            "skipping 12411\n",
            "skipping 12412\n",
            "skipping 12413\n",
            "skipping 12414\n",
            "skipping 12415\n",
            "skipping 12416\n",
            "skipping 12417\n",
            "skipping 12418\n",
            "skipping 12419\n",
            "skipping 12420\n",
            "skipping 12421\n",
            "skipping 12422\n",
            "skipping 12423\n",
            "skipping 12425\n",
            "skipping 12426\n",
            "skipping 12427\n",
            "skipping 12428\n",
            "skipping 12429\n",
            "skipping 12430\n",
            "skipping 12431\n",
            "skipping 12432\n",
            "skipping 12433\n",
            "skipping 12434\n",
            "skipping 12435\n",
            "skipping 12436\n",
            "skipping 12437\n",
            "skipping 12438\n",
            "skipping 12439\n",
            "skipping 12440\n",
            "skipping 12441\n",
            "skipping 12442\n",
            "skipping 12443\n",
            "skipping 12444\n",
            "skipping 12445\n",
            "skipping 12447\n",
            "skipping 12448\n",
            "skipping 12449\n",
            "skipping 12450\n",
            "skipping 12451\n",
            "skipping 12452\n",
            "skipping 12453\n",
            "skipping 12454\n",
            "skipping 12455\n",
            "skipping 12456\n",
            "skipping 12457\n",
            "skipping 12458\n",
            "skipping 12459\n",
            "skipping 12460\n",
            "skipping 12461\n",
            "skipping 12462\n",
            "skipping 12463\n",
            "skipping 12464\n",
            "skipping 12465\n",
            "skipping 12466\n",
            "skipping 12467\n",
            "skipping 12468\n",
            "skipping 12469\n",
            "skipping 12470\n",
            "skipping 12471\n",
            "skipping 12472\n",
            "skipping 12473\n",
            "skipping 12474\n",
            "skipping 12475\n",
            "skipping 12477\n",
            "skipping 12478\n",
            "skipping 12479\n",
            "skipping 12480\n",
            "skipping 12481\n",
            "skipping 12482\n",
            "skipping 12483\n",
            "skipping 12484\n",
            "skipping 12485\n",
            "skipping 12486\n",
            "skipping 12487\n",
            "skipping 12488\n",
            "skipping 12489\n",
            "skipping 12490\n",
            "skipping 12491\n",
            "skipping 12492\n",
            "skipping 12493\n",
            "skipping 12494\n",
            "skipping 12495\n",
            "skipping 12496\n",
            "skipping 12497\n",
            "skipping 12498\n",
            "skipping 12499\n",
            "skipping 12500\n",
            "skipping 12501\n",
            "skipping 12502\n",
            "skipping 12503\n",
            "skipping 12504\n",
            "skipping 12505\n",
            "skipping 12507\n",
            "skipping 12508\n",
            "skipping 12509\n",
            "skipping 12510\n",
            "skipping 12511\n",
            "skipping 12512\n",
            "skipping 12513\n",
            "skipping 12514\n",
            "skipping 12515\n",
            "skipping 12516\n",
            "skipping 12517\n",
            "skipping 12518\n",
            "skipping 12519\n",
            "skipping 12520\n",
            "skipping 12521\n",
            "skipping 12522\n",
            "skipping 12523\n",
            "skipping 12524\n",
            "skipping 12525\n",
            "skipping 12526\n",
            "skipping 12527\n",
            "skipping 12528\n",
            "skipping 12529\n",
            "skipping 12530\n",
            "skipping 12531\n",
            "skipping 12532\n",
            "skipping 12533\n",
            "skipping 12534\n",
            "skipping 12535\n",
            "skipping 12536\n",
            "skipping 12537\n",
            "skipping 12538\n",
            "skipping 12539\n",
            "skipping 12540\n",
            "skipping 12541\n",
            "skipping 12542\n",
            "skipping 12543\n",
            "skipping 12544\n",
            "skipping 12545\n",
            "skipping 12546\n",
            "skipping 12547\n",
            "skipping 12548\n",
            "skipping 12550\n",
            "skipping 12551\n",
            "skipping 12552\n",
            "skipping 12553\n",
            "skipping 12554\n",
            "skipping 12555\n",
            "skipping 12556\n",
            "skipping 12557\n",
            "skipping 12558\n",
            "skipping 12559\n",
            "skipping 12560\n",
            "skipping 12561\n",
            "skipping 12562\n",
            "skipping 12563\n",
            "skipping 12564\n",
            "skipping 12565\n",
            "skipping 12566\n",
            "skipping 12567\n",
            "skipping 12568\n",
            "skipping 12569\n",
            "skipping 12570\n",
            "skipping 12571\n",
            "skipping 12572\n",
            "skipping 12573\n",
            "skipping 12574\n",
            "skipping 12575\n",
            "skipping 12576\n",
            "skipping 12577\n",
            "skipping 12578\n",
            "skipping 12579\n",
            "skipping 12580\n",
            "skipping 12581\n",
            "skipping 12582\n",
            "skipping 12583\n",
            "skipping 12584\n",
            "skipping 12585\n",
            "skipping 12586\n",
            "skipping 12587\n",
            "skipping 12588\n",
            "skipping 12589\n",
            "skipping 12590\n",
            "skipping 12591\n",
            "skipping 12592\n",
            "skipping 12593\n",
            "skipping 12594\n",
            "skipping 12595\n",
            "skipping 12596\n",
            "skipping 12597\n",
            "skipping 12598\n",
            "skipping 12599\n",
            "skipping 12600\n",
            "skipping 12601\n",
            "skipping 12602\n",
            "skipping 12603\n",
            "skipping 12604\n",
            "skipping 12605\n",
            "skipping 12607\n",
            "skipping 12608\n",
            "skipping 12609\n",
            "skipping 12610\n",
            "skipping 12611\n",
            "skipping 12612\n",
            "skipping 12613\n",
            "skipping 12614\n",
            "skipping 12615\n",
            "skipping 12616\n",
            "skipping 12617\n",
            "skipping 12618\n",
            "skipping 12619\n",
            "skipping 12620\n",
            "skipping 12621\n",
            "skipping 12622\n",
            "skipping 12623\n",
            "skipping 12624\n",
            "skipping 12625\n",
            "skipping 12626\n",
            "skipping 12627\n",
            "skipping 12628\n",
            "skipping 12630\n",
            "skipping 12631\n",
            "skipping 12632\n",
            "skipping 12633\n",
            "skipping 12634\n",
            "skipping 12635\n",
            "skipping 12636\n",
            "skipping 12637\n",
            "skipping 12638\n",
            "skipping 12639\n",
            "skipping 12640\n",
            "skipping 12641\n",
            "skipping 12642\n",
            "skipping 12643\n",
            "skipping 12644\n",
            "skipping 12645\n",
            "skipping 12646\n",
            "skipping 12647\n",
            "skipping 12648\n",
            "skipping 12649\n",
            "skipping 12650\n",
            "skipping 12651\n",
            "skipping 12652\n",
            "skipping 12653\n",
            "skipping 12654\n",
            "skipping 12655\n",
            "skipping 12656\n",
            "skipping 12657\n",
            "skipping 12658\n",
            "skipping 12659\n",
            "skipping 12660\n",
            "skipping 12661\n",
            "skipping 12662\n",
            "skipping 12663\n",
            "skipping 12664\n",
            "skipping 12666\n",
            "skipping 12667\n",
            "skipping 12668\n",
            "skipping 12669\n",
            "skipping 12670\n",
            "skipping 12671\n",
            "skipping 12672\n",
            "skipping 12673\n",
            "skipping 12674\n",
            "skipping 12675\n",
            "skipping 12676\n",
            "skipping 12677\n",
            "skipping 12678\n",
            "skipping 12679\n",
            "skipping 12681\n",
            "skipping 12682\n",
            "skipping 12683\n",
            "skipping 12684\n",
            "skipping 12685\n",
            "skipping 12686\n",
            "skipping 12687\n",
            "skipping 12688\n",
            "skipping 12689\n",
            "skipping 12690\n",
            "skipping 12691\n",
            "skipping 12692\n",
            "skipping 12693\n",
            "skipping 12694\n",
            "skipping 12695\n",
            "skipping 12696\n",
            "skipping 12697\n",
            "skipping 12698\n",
            "skipping 12699\n",
            "skipping 12700\n",
            "skipping 12701\n",
            "skipping 12702\n",
            "skipping 12703\n",
            "skipping 12704\n",
            "skipping 12705\n",
            "skipping 12706\n",
            "skipping 12707\n",
            "skipping 12708\n",
            "skipping 12709\n",
            "skipping 12710\n",
            "skipping 12711\n",
            "skipping 12712\n",
            "skipping 12713\n",
            "skipping 12714\n",
            "skipping 12715\n",
            "skipping 12716\n",
            "skipping 12717\n",
            "skipping 12718\n",
            "skipping 12720\n",
            "skipping 12721\n",
            "skipping 12722\n",
            "skipping 12723\n",
            "skipping 12724\n",
            "skipping 12725\n",
            "skipping 12726\n",
            "skipping 12727\n",
            "skipping 12728\n",
            "skipping 12729\n",
            "skipping 12730\n",
            "skipping 12731\n",
            "skipping 12732\n",
            "skipping 12733\n",
            "skipping 12734\n",
            "skipping 12736\n",
            "skipping 12737\n",
            "skipping 12738\n",
            "skipping 12739\n",
            "skipping 12740\n",
            "skipping 12741\n",
            "skipping 12742\n",
            "skipping 12743\n",
            "skipping 12744\n",
            "skipping 12745\n",
            "skipping 12747\n",
            "skipping 12748\n",
            "skipping 12749\n",
            "skipping 12750\n",
            "skipping 12751\n",
            "skipping 12752\n",
            "skipping 12753\n",
            "skipping 12754\n",
            "skipping 12755\n",
            "skipping 12756\n",
            "skipping 12757\n",
            "skipping 12758\n",
            "skipping 12759\n",
            "skipping 12760\n",
            "skipping 12761\n",
            "skipping 12762\n",
            "skipping 12763\n",
            "skipping 12764\n",
            "skipping 12765\n",
            "skipping 12766\n",
            "skipping 12767\n",
            "skipping 12768\n",
            "skipping 12769\n",
            "skipping 12770\n",
            "skipping 12771\n",
            "skipping 12772\n",
            "skipping 12773\n",
            "skipping 12774\n",
            "skipping 12775\n",
            "skipping 12776\n",
            "skipping 12777\n",
            "skipping 12778\n",
            "skipping 12779\n",
            "skipping 12780\n",
            "skipping 12781\n",
            "skipping 12783\n",
            "skipping 12784\n",
            "skipping 12785\n",
            "skipping 12786\n",
            "skipping 12787\n",
            "skipping 12788\n",
            "skipping 12789\n",
            "skipping 12790\n",
            "skipping 12791\n",
            "skipping 12792\n",
            "skipping 12793\n",
            "skipping 12794\n",
            "skipping 12795\n",
            "skipping 12796\n",
            "skipping 12797\n",
            "skipping 12798\n",
            "skipping 12799\n",
            "skipping 12800\n",
            "skipping 12801\n",
            "skipping 12802\n",
            "skipping 12803\n",
            "skipping 12804\n",
            "skipping 12805\n",
            "skipping 12806\n",
            "skipping 12807\n",
            "skipping 12808\n",
            "skipping 12809\n",
            "skipping 12810\n",
            "skipping 12811\n",
            "skipping 12812\n",
            "skipping 12813\n",
            "skipping 12814\n",
            "skipping 12815\n",
            "skipping 12817\n",
            "skipping 12818\n",
            "skipping 12819\n",
            "skipping 12820\n",
            "skipping 12821\n",
            "skipping 12822\n",
            "skipping 12823\n",
            "skipping 12824\n",
            "skipping 12825\n",
            "skipping 12826\n",
            "skipping 12827\n",
            "skipping 12828\n",
            "skipping 12829\n",
            "skipping 12830\n",
            "skipping 12831\n",
            "skipping 12832\n",
            "skipping 12833\n",
            "skipping 12834\n",
            "skipping 12835\n",
            "skipping 12836\n",
            "skipping 12837\n",
            "skipping 12838\n",
            "skipping 12839\n",
            "skipping 12840\n",
            "skipping 12841\n",
            "skipping 12842\n",
            "skipping 12843\n",
            "skipping 12844\n",
            "skipping 12845\n",
            "skipping 12846\n",
            "skipping 12847\n",
            "skipping 12848\n",
            "skipping 12849\n",
            "skipping 12850\n",
            "skipping 12851\n",
            "skipping 12852\n",
            "skipping 12853\n",
            "skipping 12854\n",
            "skipping 12855\n",
            "skipping 12856\n",
            "skipping 12857\n",
            "skipping 12858\n",
            "skipping 12859\n",
            "skipping 12860\n",
            "skipping 12861\n",
            "skipping 12862\n",
            "skipping 12863\n",
            "skipping 12864\n",
            "skipping 12865\n",
            "skipping 12866\n",
            "skipping 12867\n",
            "skipping 12868\n",
            "skipping 12869\n",
            "skipping 12870\n",
            "skipping 12871\n",
            "skipping 12872\n",
            "skipping 12873\n",
            "skipping 12874\n",
            "skipping 12875\n",
            "skipping 12876\n",
            "skipping 12877\n",
            "skipping 12878\n",
            "skipping 12879\n",
            "skipping 12880\n",
            "skipping 12881\n",
            "skipping 12882\n",
            "skipping 12883\n",
            "skipping 12884\n",
            "skipping 12885\n",
            "skipping 12886\n",
            "skipping 12887\n",
            "skipping 12888\n",
            "skipping 12889\n",
            "skipping 12890\n",
            "skipping 12891\n",
            "skipping 12892\n",
            "skipping 12893\n",
            "skipping 12894\n",
            "skipping 12895\n",
            "skipping 12896\n",
            "skipping 12897\n",
            "skipping 12898\n",
            "skipping 12899\n",
            "skipping 12900\n",
            "skipping 12901\n",
            "skipping 12902\n",
            "skipping 12903\n",
            "skipping 12904\n",
            "skipping 12905\n",
            "skipping 12906\n",
            "skipping 12907\n",
            "skipping 12908\n",
            "skipping 12909\n",
            "skipping 12910\n",
            "skipping 12911\n",
            "skipping 12912\n",
            "skipping 12913\n",
            "skipping 12914\n",
            "skipping 12915\n",
            "skipping 12916\n",
            "skipping 12917\n",
            "skipping 12918\n",
            "skipping 12919\n",
            "skipping 12920\n",
            "skipping 12921\n",
            "skipping 12922\n",
            "skipping 12923\n",
            "skipping 12924\n",
            "skipping 12925\n",
            "skipping 12926\n",
            "skipping 12927\n",
            "skipping 12928\n",
            "skipping 12929\n",
            "skipping 12930\n",
            "skipping 12931\n",
            "skipping 12932\n",
            "skipping 12933\n",
            "skipping 12934\n",
            "skipping 12935\n",
            "skipping 12936\n",
            "skipping 12937\n",
            "skipping 12938\n",
            "skipping 12939\n",
            "skipping 12940\n",
            "skipping 12941\n",
            "skipping 12942\n",
            "skipping 12943\n",
            "skipping 12944\n",
            "skipping 12945\n",
            "skipping 12946\n",
            "skipping 12947\n",
            "skipping 12948\n",
            "skipping 12949\n",
            "skipping 12950\n",
            "skipping 12951\n",
            "skipping 12952\n",
            "skipping 12953\n",
            "skipping 12954\n",
            "skipping 12955\n",
            "skipping 12956\n",
            "skipping 12957\n",
            "skipping 12958\n",
            "skipping 12959\n",
            "skipping 12960\n",
            "skipping 12961\n",
            "skipping 12962\n",
            "skipping 12963\n",
            "skipping 12964\n",
            "skipping 12965\n",
            "skipping 12966\n",
            "skipping 12967\n",
            "skipping 12968\n",
            "skipping 12969\n",
            "skipping 12970\n",
            "skipping 12971\n",
            "skipping 12972\n",
            "skipping 12973\n",
            "skipping 12974\n",
            "skipping 12975\n",
            "skipping 12976\n",
            "skipping 12977\n",
            "skipping 12978\n",
            "skipping 12979\n",
            "skipping 12980\n",
            "skipping 12981\n",
            "skipping 12982\n",
            "skipping 12983\n",
            "skipping 12984\n",
            "skipping 12985\n",
            "skipping 12986\n",
            "skipping 12987\n",
            "skipping 12988\n",
            "skipping 12989\n",
            "skipping 12990\n",
            "skipping 12992\n",
            "skipping 12993\n",
            "skipping 12994\n",
            "skipping 12995\n",
            "skipping 12996\n",
            "skipping 12997\n",
            "skipping 12998\n",
            "skipping 12999\n",
            "skipping 13000\n",
            "skipping 13001\n",
            "skipping 13002\n",
            "skipping 13003\n",
            "skipping 13004\n",
            "skipping 13005\n",
            "skipping 13006\n",
            "skipping 13007\n",
            "skipping 13008\n",
            "skipping 13009\n",
            "skipping 13010\n",
            "skipping 13011\n",
            "skipping 13012\n",
            "skipping 13013\n",
            "skipping 13015\n",
            "skipping 13016\n",
            "skipping 13017\n",
            "skipping 13018\n",
            "skipping 13019\n",
            "skipping 13020\n",
            "skipping 13021\n",
            "skipping 13022\n",
            "skipping 13024\n",
            "skipping 13025\n",
            "skipping 13026\n",
            "skipping 13027\n",
            "skipping 13028\n",
            "skipping 13029\n",
            "skipping 13030\n",
            "skipping 13031\n",
            "skipping 13032\n",
            "skipping 13033\n",
            "skipping 13034\n",
            "skipping 13035\n",
            "skipping 13036\n",
            "skipping 13037\n",
            "skipping 13038\n",
            "skipping 13040\n",
            "skipping 13041\n",
            "skipping 13042\n",
            "skipping 13043\n",
            "skipping 13044\n",
            "skipping 13045\n",
            "skipping 13046\n",
            "skipping 13047\n",
            "skipping 13048\n",
            "skipping 13049\n",
            "skipping 13050\n",
            "skipping 13051\n",
            "skipping 13052\n",
            "skipping 13053\n",
            "skipping 13054\n",
            "skipping 13055\n",
            "skipping 13056\n",
            "skipping 13057\n",
            "skipping 13058\n",
            "skipping 13059\n",
            "skipping 13060\n",
            "skipping 13061\n",
            "skipping 13062\n",
            "skipping 13063\n",
            "skipping 13064\n",
            "skipping 13065\n",
            "skipping 13066\n",
            "skipping 13067\n",
            "skipping 13069\n",
            "skipping 13070\n",
            "skipping 13071\n",
            "skipping 13072\n",
            "skipping 13073\n",
            "skipping 13074\n",
            "skipping 13075\n",
            "skipping 13076\n",
            "skipping 13077\n",
            "skipping 13078\n",
            "skipping 13079\n",
            "skipping 13080\n",
            "skipping 13081\n",
            "skipping 13082\n",
            "skipping 13083\n",
            "skipping 13084\n",
            "skipping 13085\n",
            "skipping 13086\n",
            "skipping 13087\n",
            "skipping 13089\n",
            "skipping 13090\n",
            "skipping 13091\n",
            "skipping 13092\n",
            "skipping 13093\n",
            "skipping 13094\n",
            "skipping 13095\n",
            "skipping 13096\n",
            "skipping 13097\n",
            "skipping 13098\n",
            "skipping 13099\n",
            "skipping 13100\n",
            "skipping 13101\n",
            "skipping 13102\n",
            "skipping 13103\n",
            "skipping 13104\n",
            "skipping 13105\n",
            "skipping 13106\n",
            "skipping 13107\n",
            "skipping 13108\n",
            "skipping 13109\n",
            "skipping 13110\n",
            "skipping 13111\n",
            "skipping 13112\n",
            "skipping 13113\n",
            "skipping 13114\n",
            "skipping 13115\n",
            "skipping 13116\n",
            "skipping 13117\n",
            "skipping 13118\n",
            "skipping 13119\n",
            "skipping 13120\n",
            "skipping 13121\n",
            "skipping 13122\n",
            "skipping 13124\n",
            "skipping 13125\n",
            "skipping 13126\n",
            "skipping 13127\n",
            "skipping 13128\n",
            "skipping 13129\n",
            "skipping 13130\n",
            "skipping 13131\n",
            "skipping 13132\n",
            "skipping 13133\n",
            "skipping 13134\n",
            "skipping 13135\n",
            "skipping 13136\n",
            "skipping 13137\n",
            "skipping 13138\n",
            "skipping 13139\n",
            "skipping 13140\n",
            "skipping 13141\n",
            "skipping 13142\n",
            "skipping 13143\n",
            "skipping 13144\n",
            "skipping 13145\n",
            "skipping 13146\n",
            "skipping 13147\n",
            "skipping 13148\n",
            "skipping 13149\n",
            "skipping 13150\n",
            "skipping 13151\n",
            "skipping 13152\n",
            "skipping 13153\n",
            "skipping 13154\n",
            "skipping 13155\n",
            "skipping 13156\n",
            "skipping 13157\n",
            "skipping 13158\n",
            "skipping 13159\n",
            "skipping 13160\n",
            "skipping 13161\n",
            "skipping 13162\n",
            "skipping 13163\n",
            "skipping 13165\n",
            "skipping 13166\n",
            "skipping 13167\n",
            "skipping 13168\n",
            "skipping 13169\n",
            "skipping 13170\n",
            "skipping 13171\n",
            "skipping 13172\n",
            "skipping 13173\n",
            "skipping 13174\n",
            "skipping 13175\n",
            "skipping 13176\n",
            "skipping 13177\n",
            "skipping 13178\n",
            "skipping 13179\n",
            "skipping 13180\n",
            "skipping 13181\n",
            "skipping 13182\n",
            "skipping 13183\n",
            "skipping 13184\n",
            "skipping 13185\n",
            "skipping 13186\n",
            "skipping 13187\n",
            "skipping 13188\n",
            "skipping 13189\n",
            "skipping 13191\n",
            "skipping 13192\n",
            "skipping 13193\n",
            "skipping 13194\n",
            "skipping 13195\n",
            "skipping 13196\n",
            "skipping 13197\n",
            "skipping 13198\n",
            "skipping 13199\n",
            "skipping 13200\n",
            "skipping 13201\n",
            "skipping 13202\n",
            "skipping 13203\n",
            "skipping 13204\n",
            "skipping 13205\n",
            "skipping 13206\n",
            "skipping 13207\n",
            "skipping 13208\n",
            "skipping 13209\n",
            "skipping 13210\n",
            "skipping 13211\n",
            "skipping 13212\n",
            "skipping 13213\n",
            "skipping 13214\n",
            "skipping 13215\n",
            "skipping 13216\n",
            "skipping 13217\n",
            "skipping 13218\n",
            "skipping 13219\n",
            "skipping 13220\n",
            "skipping 13221\n",
            "skipping 13222\n",
            "skipping 13223\n",
            "skipping 13224\n",
            "skipping 13225\n",
            "skipping 13226\n",
            "skipping 13227\n",
            "skipping 13228\n",
            "skipping 13229\n",
            "skipping 13230\n",
            "skipping 13231\n",
            "skipping 13232\n",
            "skipping 13233\n",
            "skipping 13234\n",
            "skipping 13235\n",
            "skipping 13236\n",
            "skipping 13237\n",
            "skipping 13238\n",
            "skipping 13239\n",
            "skipping 13240\n",
            "skipping 13241\n",
            "skipping 13242\n",
            "skipping 13243\n",
            "skipping 13244\n",
            "skipping 13245\n",
            "skipping 13246\n",
            "skipping 13247\n",
            "skipping 13248\n",
            "skipping 13249\n",
            "skipping 13250\n",
            "skipping 13251\n",
            "skipping 13252\n",
            "skipping 13253\n",
            "skipping 13254\n",
            "skipping 13256\n",
            "skipping 13257\n",
            "skipping 13258\n",
            "skipping 13259\n",
            "skipping 13260\n",
            "skipping 13261\n",
            "skipping 13262\n",
            "skipping 13263\n",
            "skipping 13264\n",
            "skipping 13265\n",
            "skipping 13266\n",
            "skipping 13267\n",
            "skipping 13268\n",
            "skipping 13269\n",
            "skipping 13270\n",
            "skipping 13271\n",
            "skipping 13272\n",
            "skipping 13273\n",
            "skipping 13274\n",
            "skipping 13275\n",
            "skipping 13276\n",
            "skipping 13277\n",
            "skipping 13278\n",
            "skipping 13279\n",
            "skipping 13280\n",
            "skipping 13281\n",
            "skipping 13282\n",
            "skipping 13283\n",
            "skipping 13284\n",
            "skipping 13285\n",
            "skipping 13286\n",
            "skipping 13287\n",
            "skipping 13288\n",
            "skipping 13289\n",
            "skipping 13290\n",
            "skipping 13291\n",
            "skipping 13292\n",
            "skipping 13293\n",
            "skipping 13294\n",
            "skipping 13295\n",
            "skipping 13296\n",
            "skipping 13297\n",
            "skipping 13298\n",
            "skipping 13299\n",
            "skipping 13300\n",
            "skipping 13301\n",
            "skipping 13302\n",
            "skipping 13303\n",
            "skipping 13305\n",
            "skipping 13306\n",
            "skipping 13307\n",
            "skipping 13308\n",
            "skipping 13309\n",
            "skipping 13310\n",
            "skipping 13312\n",
            "skipping 13313\n",
            "skipping 13314\n",
            "skipping 13315\n",
            "skipping 13316\n",
            "skipping 13317\n",
            "skipping 13318\n",
            "skipping 13319\n",
            "skipping 13320\n",
            "skipping 13321\n",
            "skipping 13322\n",
            "skipping 13323\n",
            "skipping 13325\n",
            "skipping 13326\n",
            "skipping 13327\n",
            "skipping 13328\n",
            "skipping 13329\n",
            "skipping 13330\n",
            "skipping 13331\n",
            "skipping 13332\n",
            "skipping 13333\n",
            "skipping 13334\n",
            "skipping 13335\n",
            "skipping 13336\n",
            "skipping 13337\n",
            "skipping 13338\n",
            "skipping 13339\n",
            "skipping 13340\n",
            "skipping 13341\n",
            "skipping 13342\n",
            "skipping 13343\n",
            "skipping 13344\n",
            "skipping 13345\n",
            "skipping 13346\n",
            "skipping 13347\n",
            "skipping 13348\n",
            "skipping 13349\n",
            "skipping 13350\n",
            "skipping 13351\n",
            "skipping 13352\n",
            "skipping 13353\n",
            "skipping 13354\n",
            "skipping 13355\n",
            "skipping 13356\n",
            "skipping 13357\n",
            "skipping 13358\n",
            "skipping 13360\n",
            "skipping 13361\n",
            "skipping 13362\n",
            "skipping 13363\n",
            "skipping 13364\n",
            "skipping 13365\n",
            "skipping 13366\n",
            "skipping 13367\n",
            "skipping 13368\n",
            "skipping 13369\n",
            "skipping 13370\n",
            "skipping 13371\n",
            "skipping 13373\n",
            "skipping 13374\n",
            "skipping 13375\n",
            "skipping 13376\n",
            "skipping 13377\n",
            "skipping 13378\n",
            "skipping 13379\n",
            "skipping 13380\n",
            "skipping 13381\n",
            "skipping 13382\n",
            "skipping 13383\n",
            "skipping 13384\n",
            "skipping 13386\n",
            "skipping 13387\n",
            "skipping 13388\n",
            "skipping 13389\n",
            "skipping 13390\n",
            "skipping 13391\n",
            "skipping 13392\n",
            "skipping 13393\n",
            "skipping 13394\n",
            "skipping 13395\n",
            "skipping 13396\n",
            "skipping 13397\n",
            "skipping 13398\n",
            "skipping 13399\n",
            "skipping 13400\n",
            "skipping 13401\n",
            "skipping 13402\n",
            "skipping 13403\n",
            "skipping 13404\n",
            "skipping 13405\n",
            "skipping 13406\n",
            "skipping 13408\n",
            "skipping 13409\n",
            "skipping 13410\n",
            "skipping 13411\n",
            "skipping 13412\n",
            "skipping 13413\n",
            "skipping 13414\n",
            "skipping 13415\n",
            "skipping 13416\n",
            "skipping 13417\n",
            "skipping 13418\n",
            "skipping 13419\n",
            "skipping 13420\n",
            "skipping 13421\n",
            "skipping 13422\n",
            "skipping 13423\n",
            "skipping 13424\n",
            "skipping 13425\n",
            "skipping 13426\n",
            "skipping 13427\n",
            "skipping 13428\n",
            "skipping 13429\n",
            "skipping 13430\n",
            "skipping 13431\n",
            "skipping 13432\n",
            "skipping 13433\n",
            "skipping 13434\n",
            "skipping 13435\n",
            "skipping 13436\n",
            "skipping 13437\n",
            "skipping 13438\n",
            "skipping 13439\n",
            "skipping 13440\n",
            "skipping 13441\n",
            "skipping 13442\n",
            "skipping 13444\n",
            "skipping 13445\n",
            "skipping 13446\n",
            "skipping 13447\n",
            "skipping 13448\n",
            "skipping 13449\n",
            "skipping 13450\n",
            "skipping 13451\n",
            "skipping 13452\n",
            "skipping 13453\n",
            "skipping 13454\n",
            "skipping 13455\n",
            "skipping 13456\n",
            "skipping 13457\n",
            "skipping 13458\n",
            "skipping 13459\n",
            "skipping 13460\n",
            "skipping 13461\n",
            "skipping 13462\n",
            "skipping 13463\n",
            "skipping 13464\n",
            "skipping 13465\n",
            "skipping 13466\n",
            "skipping 13467\n",
            "skipping 13468\n",
            "skipping 13469\n",
            "skipping 13470\n",
            "skipping 13471\n",
            "skipping 13472\n",
            "skipping 13473\n",
            "skipping 13474\n",
            "skipping 13475\n",
            "skipping 13476\n",
            "skipping 13477\n",
            "skipping 13478\n",
            "skipping 13479\n",
            "skipping 13480\n",
            "skipping 13481\n",
            "skipping 13482\n",
            "skipping 13483\n",
            "skipping 13484\n",
            "skipping 13485\n",
            "skipping 13486\n",
            "skipping 13487\n",
            "skipping 13488\n",
            "skipping 13489\n",
            "skipping 13490\n",
            "skipping 13491\n",
            "skipping 13492\n",
            "skipping 13493\n",
            "skipping 13494\n",
            "skipping 13495\n",
            "skipping 13497\n",
            "skipping 13498\n",
            "skipping 13499\n",
            "skipping 13500\n",
            "skipping 13501\n",
            "skipping 13502\n",
            "skipping 13503\n",
            "skipping 13504\n",
            "skipping 13505\n",
            "skipping 13506\n",
            "skipping 13507\n",
            "skipping 13508\n",
            "skipping 13509\n",
            "skipping 13510\n",
            "skipping 13511\n",
            "skipping 13512\n",
            "skipping 13513\n",
            "skipping 13514\n",
            "skipping 13515\n",
            "skipping 13516\n",
            "skipping 13517\n",
            "skipping 13518\n",
            "skipping 13519\n",
            "skipping 13520\n",
            "skipping 13521\n",
            "skipping 13522\n",
            "skipping 13523\n",
            "skipping 13524\n",
            "skipping 13525\n",
            "skipping 13526\n",
            "skipping 13527\n",
            "skipping 13528\n",
            "skipping 13529\n",
            "skipping 13530\n",
            "skipping 13531\n",
            "skipping 13532\n",
            "skipping 13533\n",
            "skipping 13534\n",
            "skipping 13535\n",
            "skipping 13536\n",
            "skipping 13537\n",
            "skipping 13538\n",
            "skipping 13539\n",
            "skipping 13540\n",
            "skipping 13541\n",
            "skipping 13542\n",
            "skipping 13543\n",
            "skipping 13544\n",
            "skipping 13545\n",
            "skipping 13546\n",
            "skipping 13547\n",
            "skipping 13548\n",
            "skipping 13549\n",
            "skipping 13550\n",
            "skipping 13551\n",
            "skipping 13552\n",
            "skipping 13553\n",
            "skipping 13554\n",
            "skipping 13555\n",
            "skipping 13556\n",
            "skipping 13557\n",
            "skipping 13558\n",
            "skipping 13559\n",
            "skipping 13560\n",
            "skipping 13561\n",
            "skipping 13562\n",
            "skipping 13563\n",
            "skipping 13564\n",
            "skipping 13565\n",
            "skipping 13566\n",
            "skipping 13567\n",
            "skipping 13568\n",
            "skipping 13569\n",
            "skipping 13570\n",
            "skipping 13571\n",
            "skipping 13572\n",
            "skipping 13573\n",
            "skipping 13574\n",
            "skipping 13575\n",
            "skipping 13576\n",
            "skipping 13577\n",
            "skipping 13578\n",
            "skipping 13579\n",
            "skipping 13580\n",
            "skipping 13581\n",
            "skipping 13582\n",
            "skipping 13583\n",
            "skipping 13584\n",
            "skipping 13585\n",
            "skipping 13586\n",
            "skipping 13587\n",
            "skipping 13588\n",
            "skipping 13589\n",
            "skipping 13590\n",
            "skipping 13592\n",
            "skipping 13593\n",
            "skipping 13594\n",
            "skipping 13595\n",
            "skipping 13596\n",
            "skipping 13597\n",
            "skipping 13598\n",
            "skipping 13599\n",
            "skipping 13600\n",
            "skipping 13601\n",
            "skipping 13602\n",
            "skipping 13603\n",
            "skipping 13604\n",
            "skipping 13605\n",
            "skipping 13606\n",
            "skipping 13607\n",
            "skipping 13608\n",
            "skipping 13609\n",
            "skipping 13610\n",
            "skipping 13611\n",
            "skipping 13612\n",
            "skipping 13613\n",
            "skipping 13614\n",
            "skipping 13615\n",
            "skipping 13616\n",
            "skipping 13617\n",
            "skipping 13618\n",
            "skipping 13619\n",
            "skipping 13620\n",
            "skipping 13621\n",
            "skipping 13622\n",
            "skipping 13623\n",
            "skipping 13624\n",
            "skipping 13625\n",
            "skipping 13626\n",
            "skipping 13627\n",
            "skipping 13628\n",
            "skipping 13629\n",
            "skipping 13630\n",
            "skipping 13631\n",
            "skipping 13632\n",
            "skipping 13633\n",
            "skipping 13634\n",
            "skipping 13635\n",
            "skipping 13636\n",
            "skipping 13637\n",
            "skipping 13638\n",
            "skipping 13639\n",
            "skipping 13640\n",
            "skipping 13641\n",
            "skipping 13642\n",
            "skipping 13643\n",
            "skipping 13644\n",
            "skipping 13645\n",
            "skipping 13646\n",
            "skipping 13647\n",
            "skipping 13648\n",
            "skipping 13649\n",
            "skipping 13650\n",
            "skipping 13651\n",
            "skipping 13652\n",
            "skipping 13653\n",
            "skipping 13654\n",
            "skipping 13655\n",
            "skipping 13656\n",
            "skipping 13657\n",
            "skipping 13658\n",
            "skipping 13659\n",
            "skipping 13660\n",
            "skipping 13661\n",
            "skipping 13662\n",
            "skipping 13663\n",
            "skipping 13664\n",
            "skipping 13665\n",
            "skipping 13666\n",
            "skipping 13667\n",
            "skipping 13668\n",
            "skipping 13669\n",
            "skipping 13670\n",
            "skipping 13671\n",
            "skipping 13672\n",
            "skipping 13673\n",
            "skipping 13674\n",
            "skipping 13675\n",
            "skipping 13676\n",
            "skipping 13677\n",
            "skipping 13678\n",
            "skipping 13679\n",
            "skipping 13680\n",
            "skipping 13681\n",
            "skipping 13682\n",
            "skipping 13683\n",
            "skipping 13684\n",
            "skipping 13685\n",
            "skipping 13686\n",
            "skipping 13687\n",
            "skipping 13689\n",
            "skipping 13690\n",
            "skipping 13691\n",
            "skipping 13692\n",
            "skipping 13693\n",
            "skipping 13694\n",
            "skipping 13695\n",
            "skipping 13696\n",
            "skipping 13697\n",
            "skipping 13698\n",
            "skipping 13699\n",
            "skipping 13700\n",
            "skipping 13701\n",
            "skipping 13702\n",
            "skipping 13703\n",
            "skipping 13704\n",
            "skipping 13705\n",
            "skipping 13706\n",
            "skipping 13707\n",
            "skipping 13708\n",
            "skipping 13709\n",
            "skipping 13710\n",
            "skipping 13711\n",
            "skipping 13712\n",
            "skipping 13713\n",
            "skipping 13714\n",
            "skipping 13715\n",
            "skipping 13716\n",
            "skipping 13717\n",
            "skipping 13718\n",
            "skipping 13719\n",
            "skipping 13720\n",
            "skipping 13721\n",
            "skipping 13722\n",
            "skipping 13724\n",
            "skipping 13725\n",
            "skipping 13726\n",
            "skipping 13727\n",
            "skipping 13728\n",
            "skipping 13729\n",
            "skipping 13730\n",
            "skipping 13731\n",
            "skipping 13732\n",
            "skipping 13733\n",
            "skipping 13734\n",
            "skipping 13735\n",
            "skipping 13736\n",
            "skipping 13737\n",
            "skipping 13738\n",
            "skipping 13739\n",
            "skipping 13740\n",
            "skipping 13741\n",
            "skipping 13742\n",
            "skipping 13743\n",
            "skipping 13744\n",
            "skipping 13745\n",
            "skipping 13746\n",
            "skipping 13747\n",
            "skipping 13748\n",
            "skipping 13749\n",
            "skipping 13750\n",
            "skipping 13751\n",
            "skipping 13752\n",
            "skipping 13753\n",
            "skipping 13754\n",
            "skipping 13755\n",
            "skipping 13757\n",
            "skipping 13758\n",
            "skipping 13759\n",
            "skipping 13760\n",
            "skipping 13761\n",
            "skipping 13762\n",
            "skipping 13763\n",
            "skipping 13764\n",
            "skipping 13765\n",
            "skipping 13766\n",
            "skipping 13767\n",
            "skipping 13768\n",
            "skipping 13769\n",
            "skipping 13770\n",
            "skipping 13771\n",
            "skipping 13772\n",
            "skipping 13773\n",
            "skipping 13774\n",
            "skipping 13775\n",
            "skipping 13776\n",
            "skipping 13777\n",
            "skipping 13778\n",
            "skipping 13779\n",
            "skipping 13780\n",
            "skipping 13781\n",
            "skipping 13782\n",
            "skipping 13783\n",
            "skipping 13784\n",
            "skipping 13785\n",
            "skipping 13786\n",
            "skipping 13787\n",
            "skipping 13788\n",
            "skipping 13789\n",
            "skipping 13790\n",
            "skipping 13791\n",
            "skipping 13792\n",
            "skipping 13793\n",
            "skipping 13794\n",
            "skipping 13795\n",
            "skipping 13796\n",
            "skipping 13797\n",
            "skipping 13798\n",
            "skipping 13800\n",
            "skipping 13801\n",
            "skipping 13802\n",
            "skipping 13803\n",
            "skipping 13804\n",
            "skipping 13805\n",
            "skipping 13806\n",
            "skipping 13807\n",
            "skipping 13808\n",
            "skipping 13809\n",
            "skipping 13810\n",
            "skipping 13811\n",
            "skipping 13812\n",
            "skipping 13813\n",
            "skipping 13814\n",
            "skipping 13815\n",
            "skipping 13816\n",
            "skipping 13817\n",
            "skipping 13818\n",
            "skipping 13819\n",
            "skipping 13820\n",
            "skipping 13821\n",
            "skipping 13822\n",
            "skipping 13823\n",
            "skipping 13824\n",
            "skipping 13825\n",
            "skipping 13826\n",
            "skipping 13827\n",
            "skipping 13828\n",
            "skipping 13829\n",
            "skipping 13830\n",
            "skipping 13831\n",
            "skipping 13832\n",
            "skipping 13833\n",
            "skipping 13834\n",
            "skipping 13835\n",
            "skipping 13836\n",
            "skipping 13837\n",
            "skipping 13838\n",
            "skipping 13839\n",
            "skipping 13840\n",
            "skipping 13841\n",
            "skipping 13842\n",
            "skipping 13843\n",
            "skipping 13844\n",
            "skipping 13845\n",
            "skipping 13846\n",
            "skipping 13847\n",
            "skipping 13848\n",
            "skipping 13849\n",
            "skipping 13850\n",
            "skipping 13851\n",
            "skipping 13852\n",
            "skipping 13853\n",
            "skipping 13854\n",
            "skipping 13855\n",
            "skipping 13856\n",
            "skipping 13857\n",
            "skipping 13858\n",
            "skipping 13859\n",
            "skipping 13860\n",
            "skipping 13861\n",
            "skipping 13862\n",
            "skipping 13863\n",
            "skipping 13864\n",
            "skipping 13865\n",
            "skipping 13866\n",
            "skipping 13867\n",
            "skipping 13868\n",
            "skipping 13869\n",
            "skipping 13870\n",
            "skipping 13871\n",
            "skipping 13872\n",
            "skipping 13873\n",
            "skipping 13874\n",
            "skipping 13875\n",
            "skipping 13876\n",
            "skipping 13877\n",
            "skipping 13878\n",
            "skipping 13879\n",
            "skipping 13880\n",
            "skipping 13881\n",
            "skipping 13882\n",
            "skipping 13883\n",
            "skipping 13884\n",
            "skipping 13885\n",
            "skipping 13886\n",
            "skipping 13887\n",
            "skipping 13888\n",
            "skipping 13890\n",
            "skipping 13891\n",
            "skipping 13892\n",
            "skipping 13893\n",
            "skipping 13894\n",
            "skipping 13895\n",
            "skipping 13896\n",
            "skipping 13897\n",
            "skipping 13898\n",
            "skipping 13899\n",
            "skipping 13900\n",
            "skipping 13901\n",
            "skipping 13902\n",
            "skipping 13903\n",
            "skipping 13904\n",
            "skipping 13905\n",
            "skipping 13906\n",
            "skipping 13907\n",
            "skipping 13908\n",
            "skipping 13909\n",
            "skipping 13910\n",
            "skipping 13911\n",
            "skipping 13912\n",
            "skipping 13913\n",
            "skipping 13914\n",
            "skipping 13915\n",
            "skipping 13917\n",
            "skipping 13918\n",
            "skipping 13919\n",
            "skipping 13920\n",
            "skipping 13921\n",
            "skipping 13922\n",
            "skipping 13923\n",
            "skipping 13924\n",
            "skipping 13925\n",
            "skipping 13926\n",
            "skipping 13927\n",
            "skipping 13928\n",
            "skipping 13929\n",
            "skipping 13930\n",
            "skipping 13931\n",
            "skipping 13932\n",
            "skipping 13933\n",
            "skipping 13934\n",
            "skipping 13935\n",
            "skipping 13936\n",
            "skipping 13937\n",
            "skipping 13938\n",
            "skipping 13939\n",
            "skipping 13940\n",
            "skipping 13941\n",
            "skipping 13942\n",
            "skipping 13943\n",
            "skipping 13944\n",
            "skipping 13945\n",
            "skipping 13946\n",
            "skipping 13947\n",
            "skipping 13948\n",
            "skipping 13949\n",
            "skipping 13950\n",
            "skipping 13951\n",
            "skipping 13952\n",
            "skipping 13953\n",
            "skipping 13954\n",
            "skipping 13956\n",
            "skipping 13957\n",
            "skipping 13958\n",
            "skipping 13959\n",
            "skipping 13960\n",
            "skipping 13961\n",
            "skipping 13962\n",
            "skipping 13963\n",
            "skipping 13964\n",
            "skipping 13965\n",
            "skipping 13966\n",
            "skipping 13967\n",
            "skipping 13968\n",
            "skipping 13969\n",
            "skipping 13970\n",
            "skipping 13971\n",
            "skipping 13972\n",
            "skipping 13973\n",
            "skipping 13974\n",
            "skipping 13975\n",
            "skipping 13976\n",
            "skipping 13977\n",
            "skipping 13978\n",
            "skipping 13979\n",
            "skipping 13980\n",
            "skipping 13981\n",
            "skipping 13982\n",
            "skipping 13983\n",
            "skipping 13984\n",
            "skipping 13985\n",
            "skipping 13986\n",
            "skipping 13987\n",
            "skipping 13988\n",
            "skipping 13989\n",
            "skipping 13990\n",
            "skipping 13991\n",
            "skipping 13992\n",
            "skipping 13993\n",
            "skipping 13994\n",
            "skipping 13996\n",
            "skipping 13997\n",
            "skipping 13998\n",
            "skipping 13999\n",
            "skipping 14000\n",
            "skipping 14001\n",
            "skipping 14002\n",
            "skipping 14003\n",
            "skipping 14004\n",
            "skipping 14005\n",
            "skipping 14006\n",
            "skipping 14007\n",
            "skipping 14008\n",
            "skipping 14009\n",
            "skipping 14010\n",
            "skipping 14011\n",
            "skipping 14013\n",
            "skipping 14014\n",
            "skipping 14015\n",
            "skipping 14016\n",
            "skipping 14017\n",
            "skipping 14018\n",
            "skipping 14019\n",
            "skipping 14020\n",
            "skipping 14021\n",
            "skipping 14022\n",
            "skipping 14023\n",
            "skipping 14024\n",
            "skipping 14025\n",
            "skipping 14026\n",
            "skipping 14027\n",
            "skipping 14028\n",
            "skipping 14029\n",
            "skipping 14030\n",
            "skipping 14031\n",
            "skipping 14032\n",
            "skipping 14033\n",
            "skipping 14034\n",
            "skipping 14035\n",
            "skipping 14036\n",
            "skipping 14037\n",
            "skipping 14038\n",
            "skipping 14039\n",
            "skipping 14040\n",
            "skipping 14041\n",
            "skipping 14042\n",
            "skipping 14043\n",
            "skipping 14044\n",
            "skipping 14045\n",
            "skipping 14046\n",
            "skipping 14047\n",
            "skipping 14048\n",
            "skipping 14049\n",
            "skipping 14050\n",
            "skipping 14051\n",
            "skipping 14053\n",
            "skipping 14054\n",
            "skipping 14055\n",
            "skipping 14056\n",
            "skipping 14057\n",
            "skipping 14058\n",
            "skipping 14059\n",
            "skipping 14060\n",
            "skipping 14061\n",
            "skipping 14062\n",
            "skipping 14063\n",
            "skipping 14064\n",
            "skipping 14065\n",
            "skipping 14066\n",
            "skipping 14067\n",
            "skipping 14068\n",
            "skipping 14069\n",
            "skipping 14070\n",
            "skipping 14071\n",
            "skipping 14072\n",
            "skipping 14073\n",
            "skipping 14074\n",
            "skipping 14075\n",
            "skipping 14076\n",
            "skipping 14077\n",
            "skipping 14078\n",
            "skipping 14079\n",
            "skipping 14080\n",
            "skipping 14081\n",
            "skipping 14082\n",
            "skipping 14083\n",
            "skipping 14084\n",
            "skipping 14085\n",
            "skipping 14086\n",
            "skipping 14087\n",
            "skipping 14088\n",
            "skipping 14089\n",
            "skipping 14090\n",
            "skipping 14091\n",
            "skipping 14092\n",
            "skipping 14093\n",
            "skipping 14094\n",
            "skipping 14095\n",
            "skipping 14096\n",
            "skipping 14098\n",
            "skipping 14099\n",
            "skipping 14100\n",
            "skipping 14101\n",
            "skipping 14102\n",
            "skipping 14103\n",
            "skipping 14104\n",
            "skipping 14105\n",
            "skipping 14106\n",
            "skipping 14107\n",
            "skipping 14108\n",
            "skipping 14109\n",
            "skipping 14110\n",
            "skipping 14111\n",
            "skipping 14112\n",
            "skipping 14113\n",
            "skipping 14114\n",
            "skipping 14115\n",
            "skipping 14116\n",
            "skipping 14117\n",
            "skipping 14118\n",
            "skipping 14119\n",
            "skipping 14120\n",
            "skipping 14121\n",
            "skipping 14122\n",
            "skipping 14123\n",
            "skipping 14124\n",
            "skipping 14126\n",
            "skipping 14127\n",
            "skipping 14128\n",
            "skipping 14129\n",
            "skipping 14131\n",
            "skipping 14132\n",
            "skipping 14133\n",
            "skipping 14134\n",
            "skipping 14135\n",
            "skipping 14136\n",
            "skipping 14137\n",
            "skipping 14138\n",
            "skipping 14139\n",
            "skipping 14140\n",
            "skipping 14141\n",
            "skipping 14142\n",
            "skipping 14143\n",
            "skipping 14144\n",
            "skipping 14145\n",
            "skipping 14146\n",
            "skipping 14147\n",
            "skipping 14148\n",
            "skipping 14149\n",
            "skipping 14150\n",
            "skipping 14151\n",
            "skipping 14152\n",
            "skipping 14153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEDi0Ow51tZ6"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "! cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duGbDrKpdbyn",
        "outputId": "ebdb2807-e612-4daf-e6cd-16c14e97436b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!bash /content/GeDi/scripts/run_training.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-05 08:43:00.136525: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "10/05/2020 08:43:01 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "10/05/2020 08:43:02 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "10/05/2020 08:43:02 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/05/2020 08:43:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "10/05/2020 08:43:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "10/05/2020 08:43:03 - INFO - modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "10/05/2020 08:43:08 - INFO - modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['logit_scale', 'lm_head.weight']\n",
            "10/05/2020 08:43:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, add_sep=False, cache_dir='', code_0='false', code_1='true', config_name='', data_dir='/content/GeDi/data/AG-news', device=device(type='cuda'), disc_weight=0.19999999999999996, do_eval=True, do_lower_case=False, do_train=True, dropout=0.1, eval_all_checkpoints=False, evaluate_during_training=False, fp16=True, fp16_opt_level='O1', gen_weight=0.8, gradient_accumulation_steps=1, jigsaw=False, jigsaw_no_toxic_gen=False, learning_rate=2e-05, local_rank=-1, logging_steps=500, logit_scale=True, mask_eos_token=False, max_grad_norm=1.0, max_seq_length=192, max_steps=-1, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, outbias=False, output_dir='/content/topic_GeDi_retrained', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=4, save_steps=5000000000, seed=42, server_ip='', server_port='', sst5=False, sum_loss=False, task_name='sst-2', threeway=False, tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "10/05/2020 08:43:13 - INFO - __main__ -   Creating features from dataset file at /content/GeDi/data/AG-news\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   Writing example 0/686\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   input_ids: 13926 873 7120 1475 3846 1387 1770 13 3271 10821 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   input_ids: 32945 3666 5891 4290 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   input_ids: 6894 11028 8835 447 247 903 20251 1737 11 5537 4139 286 1578 7526 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   input_ids: 22680 7120 1475 3846 1387 1992 38758 376 1078 993 978 311 23267 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   input_ids: 13926 873 36705 11 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:43:13 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:43:13 - INFO - __main__ -   Saving features into cached file /content/GeDi/data/AG-news/cached_train_gpt2_192_sst-2\n",
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "10/05/2020 08:43:14 - INFO - __main__ -   ***** Running training *****\n",
            "10/05/2020 08:43:14 - INFO - __main__ -     Num examples = 686\n",
            "10/05/2020 08:43:14 - INFO - __main__ -     Num Epochs = 1\n",
            "10/05/2020 08:43:14 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "10/05/2020 08:43:14 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "10/05/2020 08:43:14 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/05/2020 08:43:14 - INFO - __main__ -     Total optimization steps = 172\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  return orig_fn(arg0, *args, **kwargs)\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Epoch: 100% 1/1 [00:49<00:00, 49.29s/it]\n",
            "10/05/2020 08:44:03 - INFO - __main__ -    global_step = 172, average loss = 3.0525063456490984\n",
            "10/05/2020 08:44:03 - INFO - __main__ -   Saving model checkpoint to /content/topic_GeDi_retrained\n",
            "10/05/2020 08:44:03 - INFO - transformers.configuration_utils -   Configuration saved in /content/topic_GeDi_retrained/config.json\n",
            "10/05/2020 08:44:05 - INFO - modeling_utils -   Model weights saved in /content/topic_GeDi_retrained/pytorch_model.bin\n",
            "10/05/2020 08:44:05 - INFO - transformers.configuration_utils -   loading configuration file /content/topic_GeDi_retrained/config.json\n",
            "10/05/2020 08:44:05 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"logit_scale\": true,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"nbias\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/05/2020 08:44:05 - INFO - modeling_utils -   loading weights file /content/topic_GeDi_retrained/pytorch_model.bin\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   Model name '/content/topic_GeDi_retrained' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/topic_GeDi_retrained' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   Didn't find file /content/topic_GeDi_retrained/added_tokens.json. We won't load it.\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/vocab.json\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/merges.txt\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file None\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/special_tokens_map.json\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/tokenizer_config.json\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   Model name '/content/topic_GeDi_retrained' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/topic_GeDi_retrained' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   Didn't find file /content/topic_GeDi_retrained/added_tokens.json. We won't load it.\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/vocab.json\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/merges.txt\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file None\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/special_tokens_map.json\n",
            "10/05/2020 08:44:10 - INFO - transformers.tokenization_utils -   loading file /content/topic_GeDi_retrained/tokenizer_config.json\n",
            "10/05/2020 08:44:10 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/topic_GeDi_retrained']\n",
            "10/05/2020 08:44:10 - INFO - transformers.configuration_utils -   loading configuration file /content/topic_GeDi_retrained/config.json\n",
            "10/05/2020 08:44:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"logit_scale\": true,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"nbias\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/05/2020 08:44:10 - INFO - modeling_utils -   loading weights file /content/topic_GeDi_retrained/pytorch_model.bin\n",
            "10/05/2020 08:44:15 - INFO - __main__ -   Creating features from dataset file at /content/GeDi/data/AG-news\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   Writing example 0/296\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   input_ids: 16801 1026 3607 502 1049 9476 284 307 994 319 262 6695 286 262 23166 286 3555 1227 14395 13 314 5875 290 43647 262 350 13 45 13 5961 15799 8489 329 46729 428 13 1318 460 307 645 8716 3744 621 3555 290 645 4202 3744 621 3725 13 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   input_ids: 6894 45 321 2093 283 14213 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   input_ids: 22680 7120 1475 3846 1387 5537 4139 11366 18133 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   input_ids: 13926 873 45 321 4594 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   input_ids: 32945 3666 23420 29764 18029 11 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/05/2020 08:44:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "10/05/2020 08:44:15 - INFO - __main__ -   Saving features into cached file /content/GeDi/data/AG-news/cached_dev_gpt2_192_sst-2\n",
            "10/05/2020 08:44:15 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/05/2020 08:44:15 - INFO - __main__ -     Num examples = 296\n",
            "10/05/2020 08:44:15 - INFO - __main__ -     Batch size = 8\n",
            "{'acc': 0.5168918918918919, 'overall_gen_loss': 3.001686025310207}\n",
            "{'acc': 0.5168918918918919}\n",
            "10/05/2020 08:44:22 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/05/2020 08:44:22 - INFO - __main__ -     acc = 0.5168918918918919\n",
            "10/05/2020 08:44:22 - INFO - __main__ -     overall_gen_loss = 3.001686025310207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7ohcjTJ8Jhb",
        "outputId": "7d1eb4b8-a810-41e6-87cb-49a8f6e1e603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!bash /content/GeDi/scripts/get_models.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-05 08:46:35--  https://storage.googleapis.com/sfr-gedi-data/gedi_topic.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.195.128, 74.125.28.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318630072 (1.2G) [application/zip]\n",
            "Saving to: ‘gedi_topic.zip’\n",
            "\n",
            "gedi_topic.zip      100%[===================>]   1.23G   154MB/s    in 8.5s    \n",
            "\n",
            "2020-10-05 08:46:44 (148 MB/s) - ‘gedi_topic.zip’ saved [1318630072/1318630072]\n",
            "\n",
            "Archive:  gedi_topic.zip\n",
            "   creating: gedi_topic/\n",
            " extracting: gedi_topic/tokenizer_config.json  \n",
            "  inflating: gedi_topic/special_tokens_map.json  \n",
            "  inflating: gedi_topic/config.json  \n",
            "  inflating: gedi_topic/merges.txt   \n",
            "  inflating: gedi_topic/training_args.bin  \n",
            "  inflating: gedi_topic/pytorch_model.bin  \n",
            "  inflating: gedi_topic/vocab.json   \n",
            "  inflating: gedi_topic/eval_results.txt  \n",
            "--2020-10-05 08:47:17--  https://storage.googleapis.com/sfr-gedi-data/gedi_sentiment.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.195.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318619163 (1.2G) [application/zip]\n",
            "Saving to: ‘gedi_sentiment.zip’\n",
            "\n",
            "gedi_sentiment.zip  100%[===================>]   1.23G  32.2MB/s    in 31s     \n",
            "\n",
            "2020-10-05 08:47:48 (41.0 MB/s) - ‘gedi_sentiment.zip’ saved [1318619163/1318619163]\n",
            "\n",
            "Archive:  gedi_sentiment.zip\n",
            "   creating: gedi_sentiment/\n",
            " extracting: gedi_sentiment/tokenizer_config.json  \n",
            "  inflating: gedi_sentiment/special_tokens_map.json  \n",
            "  inflating: gedi_sentiment/config.json  \n",
            "  inflating: gedi_sentiment/merges.txt  \n",
            "  inflating: gedi_sentiment/training_args.bin  \n",
            "  inflating: gedi_sentiment/pytorch_model.bin  \n",
            "  inflating: gedi_sentiment/vocab.json  \n",
            "--2020-10-05 08:48:31--  https://storage.googleapis.com/sfr-gedi-data/gedi_detoxifier.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 74.125.28.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318622316 (1.2G) [application/zip]\n",
            "Saving to: ‘gedi_detoxifier.zip’\n",
            "\n",
            "gedi_detoxifier.zip 100%[===================>]   1.23G  38.0MB/s    in 28s     \n",
            "\n",
            "2020-10-05 08:48:59 (44.9 MB/s) - ‘gedi_detoxifier.zip’ saved [1318622316/1318622316]\n",
            "\n",
            "Archive:  gedi_detoxifier.zip\n",
            "   creating: gedi_detoxifier/\n",
            "  inflating: gedi_detoxifier/special_tokens_map.json  \n",
            "  inflating: gedi_detoxifier/vocab.json  \n",
            " extracting: gedi_detoxifier/tokenizer_config.json  \n",
            "  inflating: gedi_detoxifier/merges.txt  \n",
            "  inflating: gedi_detoxifier/training_args.bin  \n",
            " extracting: gedi_detoxifier/eval_results.txt  \n",
            "  inflating: gedi_detoxifier/config.json  \n",
            "  inflating: gedi_detoxifier/pytorch_model.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYPjOKHUJtu_",
        "outputId": "a65a4e35-d47d-466f-c679-7ef63c852263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "! bash /content/GeDi/scripts/run_generation.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-05 09:01:19.716739: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Generates positive by default. Press 'enter' to continue or 'n' to switch to negative (Warning, negative can lead to toxic generations at a higher rate than normal GPT-2 (or GPT-3)): n\n",
            "Give a generation prompt (or type q to quit): Hindus are against Muslims but\n",
            "GeDi estimates the probability that it sample is desired class is: 0.3770562410354614\n",
            "\n",
            "\n",
            "Hindus are against Muslims but not against non-Muslims,\" the party's National Conference Committee (NCPC) has told The Hindu.\n",
            "\n",
            "\"Narendra Modi, who is also an NCP member, is in charge of this programme and must be brought to a close by his leadership team,\" NCPC said in a statement on Wednesday. \"His presence on this programme has given rise to the idea that we should be looking for other candidates from other religions or sects. This programme is one of the most ambitious I have ever seen and I am very much seeking my own countrymen from other religions or sects, even though these candidates don't belong to our religion but belong to some political party leaders present there. In May 2015 when he visited Gujarat for two days with BJP leader Manohar Lal Khattakkara, Namaskar Kulkarni Jain Shri Ram Singh Bharti Swamy Singh Bhargava Samiti Rajya Sabha MP Subhash Gupta<|endoftext|>\n",
            "\n",
            "\n",
            "Generates positive by default. Press 'enter' to continue or 'n' to switch to negative (Warning, negative can lead to toxic generations at a higher rate than normal GPT-2 (or GPT-3)): "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fq82EAp8Bmw",
        "outputId": "af1ab590-3ad1-4f08-aee0-f5868d10fc83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! zip /content/data_modi_speech.zip /content/GeDi/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/GeDi/data/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joejDOTV-6bU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}